{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signal-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "working-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "enhanced-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples= tfds.load('ted_multi_translate')\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hidden-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a):\n",
    "    if a <3:\n",
    "        return 3\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "disabled-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mechanical-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afraid-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'ar' b'bg' b'de' b'en' b'es' b'fa' b'fr' b'gl' b'he' b'hr' b'hu' b'id'\n",
      " b'it' b'ja' b'ko' b'nl' b'pl' b'pt-br' b'ro' b'ru' b'th' b'tr' b'vi'\n",
      " b'zh-cn' b'zh-tw'], shape=(25,), dtype=string)\n",
      "1\n",
      "************************************************\n",
      "tf.Tensor([b'en'], shape=(1,), dtype=string)\n",
      "0\n",
      "************************************************\n",
      "tf.Tensor(\n",
      "[b'ar' b'bg' b'cs' b'de' b'el' b'en' b'es' b'eu' b'fa' b'fr' b'he' b'hr'\n",
      " b'hu' b'it' b'ja' b'ko' b'nl' b'pl' b'pt-br' b'ro' b'ru' b'sk' b'sq'\n",
      " b'sr' b'sv' b'th' b'tr' b'uk' b'vi' b'zh-cn' b'zh-tw'], shape=(31,), dtype=string)\n",
      "1\n",
      "************************************************\n"
     ]
    }
   ],
   "source": [
    "for item in train_examples.take(3):\n",
    "    #print(item)\n",
    "    print(item[\"translations\"][\"language\"])\n",
    "    print(len(tf.where(item[\"translations\"][\"language\"]==b\"fr\").numpy()))\n",
    "    print(\"************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "enabling-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def get_sentence(df):\n",
    "    if len(tf.where(df[\"translations\"][\"language\"]==b\"fr\").numpy()):\n",
    "        lang_idx_fr = tf.where(df[\"translations\"][\"language\"]==b\"fr\").numpy()[0][0]\n",
    "        if len(tf.where(df[\"translations\"][\"language\"]==b\"en\").numpy()):\n",
    "            lang_idx_en = tf.where(df[\"translations\"][\"language\"]==b\"en\").numpy()[0][0]\n",
    "            return df['translations']['translation'][lang_idx_fr], df['translations']['translation'][lang_idx_en]\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "modified-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "train_set[\"en\"]=[]\n",
    "train_set[\"fr\"]=[]\n",
    "for item in train_examples:\n",
    "    get_sentence(item)\n",
    "    if get_sentence(item) is not None:\n",
    "        train_set[\"fr\"].append(get_sentence(item)[0].numpy())\n",
    "        train_set[\"en\"].append(get_sentence(item)[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "arctic-pleasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192304"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "asian-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fr = tf.data.Dataset.from_tensor_slices(train_set[\"fr\"])\n",
    "train_en = tf.data.Dataset.from_tensor_slices(train_set[\"en\"])\n",
    "train_set = tf.data.Dataset.zip((train_fr,train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fantastic-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et je ne sais pas si \\xc3\\xa7a vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arr\\xc3\\xaate pas de penser \\xc3\\xa0 mes yeux .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et ainsi donc , le temps \\xc3\\xa9tant compt\\xc3\\xa9 , Si je vous joue litt\\xc3\\xa9ralement juste les 2 premi\\xc3\\xa8res lignes . C&apos; est tr\\xc3\\xa8s simple .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'M\\xc3\\xaame dans les pays du monde qui ont les meilleures ressources , cet \\xc3\\xa9cart d&apos; esp\\xc3\\xa9rance de vie est de 20 ans .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .'>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "consolidated-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "partial-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "animated-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 302 ms, total: 1min 38s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_fr.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "common-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 140 ms, total: 1min 17s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "public-pottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '£',\n",
       " '¿',\n",
       " 'æ',\n",
       " 'ø',\n",
       " 'τ',\n",
       " 'ย',\n",
       " 'ร',\n",
       " 'อ',\n",
       " '–',\n",
       " '—',\n",
       " '’',\n",
       " '•',\n",
       " '∇',\n",
       " '♪',\n",
       " '♫',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'apos',\n",
       " 'of',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'you',\n",
       " 'we',\n",
       " 'is',\n",
       " 'quot',\n",
       " 'this',\n",
       " 'so',\n",
       " 'they',\n",
       " 'was',\n",
       " 'for',\n",
       " '##s',\n",
       " 'are',\n",
       " 'have',\n",
       " 'what',\n",
       " 'but',\n",
       " 'on',\n",
       " 'with',\n",
       " 'can',\n",
       " 'there',\n",
       " 'about',\n",
       " 'be',\n",
       " 'as',\n",
       " 'at',\n",
       " 'all',\n",
       " 'not',\n",
       " 'do',\n",
       " 'one',\n",
       " 'my',\n",
       " 're',\n",
       " 'people',\n",
       " 'like',\n",
       " 'from',\n",
       " 'if',\n",
       " 'now',\n",
       " 'our',\n",
       " 'just',\n",
       " 'these',\n",
       " 'an',\n",
       " 'he',\n",
       " 'or',\n",
       " 'when',\n",
       " 'very',\n",
       " 'because',\n",
       " 'out',\n",
       " 'me',\n",
       " 'by',\n",
       " 'going',\n",
       " 'how',\n",
       " 'know',\n",
       " 'up',\n",
       " 'them',\n",
       " 'more',\n",
       " 'had',\n",
       " 'see',\n",
       " 'think',\n",
       " 'were',\n",
       " 'which',\n",
       " 'here',\n",
       " 'their',\n",
       " 'who',\n",
       " 'really',\n",
       " 'would',\n",
       " 'your',\n",
       " 'get',\n",
       " 'then',\n",
       " 've',\n",
       " 'us',\n",
       " 'time',\n",
       " '##ing',\n",
       " 'world',\n",
       " 'some',\n",
       " 'has',\n",
       " 'actually',\n",
       " 'don',\n",
       " 'into',\n",
       " 'where',\n",
       " 'will',\n",
       " 'way',\n",
       " '##ed',\n",
       " 'years',\n",
       " 'things',\n",
       " 'laughter',\n",
       " 'other',\n",
       " 'well',\n",
       " 'could',\n",
       " 'go',\n",
       " 'no',\n",
       " 'been',\n",
       " 'want',\n",
       " 'make',\n",
       " 'right',\n",
       " 'those',\n",
       " 'first',\n",
       " 'something',\n",
       " 'she',\n",
       " 'two',\n",
       " 'much',\n",
       " 'look',\n",
       " 'than',\n",
       " '##d',\n",
       " 'said',\n",
       " 'also',\n",
       " 'new',\n",
       " 'little',\n",
       " 'thing',\n",
       " 'got',\n",
       " 'back',\n",
       " 'over',\n",
       " 'most',\n",
       " 'even',\n",
       " 'his',\n",
       " 'life',\n",
       " 'take',\n",
       " 'only',\n",
       " 'work',\n",
       " 'say',\n",
       " '##ly',\n",
       " 'many',\n",
       " 'need',\n",
       " 'kind',\n",
       " 'did',\n",
       " 'lot',\n",
       " 'around',\n",
       " 'applause',\n",
       " 'different',\n",
       " 'why',\n",
       " 'good',\n",
       " 'every',\n",
       " 'let',\n",
       " 'down',\n",
       " 'through',\n",
       " 'her',\n",
       " 'll',\n",
       " 'same',\n",
       " 'come',\n",
       " 'being',\n",
       " 'year',\n",
       " 'three',\n",
       " 'doing',\n",
       " 'use',\n",
       " 'day',\n",
       " 'put',\n",
       " 'called',\n",
       " 'today',\n",
       " 'percent',\n",
       " 'thank',\n",
       " 'any',\n",
       " 'made',\n",
       " '##er',\n",
       " '##y',\n",
       " 'after',\n",
       " 'human',\n",
       " 'great',\n",
       " 'tell',\n",
       " 'find',\n",
       " 'fact',\n",
       " 'its',\n",
       " '##e',\n",
       " 'change',\n",
       " 'another',\n",
       " 'own',\n",
       " 'talk',\n",
       " 'didn',\n",
       " 'idea',\n",
       " 'big',\n",
       " 'last',\n",
       " 'started',\n",
       " 'before',\n",
       " 'should',\n",
       " '##a',\n",
       " '000',\n",
       " 'never',\n",
       " 'better',\n",
       " 'together',\n",
       " 'important',\n",
       " 'went',\n",
       " 'give',\n",
       " 'might',\n",
       " 'problem',\n",
       " 'thought',\n",
       " 'part',\n",
       " 'able',\n",
       " 'system',\n",
       " 'off',\n",
       " 'does',\n",
       " 'still',\n",
       " 'again',\n",
       " 'course',\n",
       " 'story',\n",
       " 'each',\n",
       " '##r',\n",
       " 'next',\n",
       " '##t',\n",
       " 'start',\n",
       " 'ago',\n",
       " 'few',\n",
       " 'long',\n",
       " 'technology',\n",
       " 'him',\n",
       " 'show',\n",
       " 'came',\n",
       " 'brain',\n",
       " 'place',\n",
       " 'bit',\n",
       " 'used',\n",
       " 'mean',\n",
       " '##n',\n",
       " 'between',\n",
       " 'old',\n",
       " 'example',\n",
       " 'water',\n",
       " '##es',\n",
       " 'too',\n",
       " 'data',\n",
       " 'question',\n",
       " 'maybe',\n",
       " 'end',\n",
       " 'looking',\n",
       " '##al',\n",
       " 'done',\n",
       " '10',\n",
       " 'found',\n",
       " 'women',\n",
       " 'doesn',\n",
       " 'point',\n",
       " 'children',\n",
       " 'understand',\n",
       " 'love',\n",
       " 'real',\n",
       " 'wanted',\n",
       " 'live',\n",
       " 'sort',\n",
       " 'four',\n",
       " 'may',\n",
       " 'ever',\n",
       " 'away',\n",
       " 'always',\n",
       " 'million',\n",
       " 'school',\n",
       " 'whole',\n",
       " 'call',\n",
       " 'trying',\n",
       " 'everything',\n",
       " 'working',\n",
       " 'person',\n",
       " 'believe',\n",
       " 'country',\n",
       " 'try',\n",
       " 'using',\n",
       " 'information',\n",
       " 'help',\n",
       " 'second',\n",
       " 'five',\n",
       " 'space',\n",
       " 'thinking',\n",
       " 'power',\n",
       " '##o',\n",
       " 'man',\n",
       " 'times',\n",
       " 'feel',\n",
       " 'high',\n",
       " 'means',\n",
       " 'number',\n",
       " 'took',\n",
       " 'small',\n",
       " 'design',\n",
       " 'create',\n",
       " 'future',\n",
       " 'kids',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'become',\n",
       " 'money',\n",
       " 'quite',\n",
       " 'building',\n",
       " 'music',\n",
       " 'city',\n",
       " 'left',\n",
       " '##ers',\n",
       " 'sense',\n",
       " 'home',\n",
       " 'without',\n",
       " 'getting',\n",
       " 'earth',\n",
       " 'best',\n",
       " 'food',\n",
       " 'comes',\n",
       " 'body',\n",
       " 'energy',\n",
       " 'happened',\n",
       " '##l',\n",
       " 'social',\n",
       " 'talking',\n",
       " 'probably',\n",
       " 'light',\n",
       " 'less',\n",
       " 'interesting',\n",
       " 'pretty',\n",
       " 'science',\n",
       " 'coming',\n",
       " 'stuff',\n",
       " 'half',\n",
       " 'am',\n",
       " 'video',\n",
       " 'imagine',\n",
       " 'across',\n",
       " 'ask',\n",
       " 'such',\n",
       " 'lives',\n",
       " '##i',\n",
       " '20',\n",
       " 'anything',\n",
       " 'told',\n",
       " 'countries',\n",
       " 'simple',\n",
       " 'dollars',\n",
       " 'play',\n",
       " 'africa',\n",
       " 'having',\n",
       " 'moment',\n",
       " 'health',\n",
       " 'happen',\n",
       " 'living',\n",
       " 'hard',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'build',\n",
       " 'ways',\n",
       " 'computer',\n",
       " 'project',\n",
       " 'cells',\n",
       " 'family',\n",
       " 'almost',\n",
       " 'okay',\n",
       " 'yet',\n",
       " 'side',\n",
       " 'makes',\n",
       " 'room',\n",
       " 'case',\n",
       " 'experience',\n",
       " 'while',\n",
       " '##ic',\n",
       " 'care',\n",
       " 'young',\n",
       " 'days',\n",
       " 'picture',\n",
       " 'later',\n",
       " 'once',\n",
       " 'far',\n",
       " 'seen',\n",
       " 'asked',\n",
       " 'process',\n",
       " 'remember',\n",
       " 'states',\n",
       " 'move',\n",
       " 'inside',\n",
       " 'nothing',\n",
       " 'goes',\n",
       " 'open',\n",
       " 'whether',\n",
       " 'billion',\n",
       " 'happens',\n",
       " 'learn',\n",
       " 'planet',\n",
       " 'basically',\n",
       " 'six',\n",
       " 'reason',\n",
       " 'says',\n",
       " 'single',\n",
       " 'both',\n",
       " 'else',\n",
       " 'global',\n",
       " 'car',\n",
       " '##able',\n",
       " 'problems',\n",
       " 'set',\n",
       " 'often',\n",
       " 'possible',\n",
       " 'bad',\n",
       " 'child',\n",
       " 'community',\n",
       " 'mind',\n",
       " 'already',\n",
       " 'within',\n",
       " 'history',\n",
       " '##m',\n",
       " 'public',\n",
       " 'ideas',\n",
       " '##on',\n",
       " 'someone',\n",
       " 'war',\n",
       " 'looked',\n",
       " 'sure',\n",
       " 'keep',\n",
       " 'men',\n",
       " 'self',\n",
       " 'everybody',\n",
       " 'hope',\n",
       " 'amazing',\n",
       " 'answer',\n",
       " 'business',\n",
       " 'looks',\n",
       " 'face',\n",
       " 'saying',\n",
       " 'matter',\n",
       " 'myself',\n",
       " '##ation',\n",
       " 'months',\n",
       " '100',\n",
       " 'instead',\n",
       " 'isn',\n",
       " 'since',\n",
       " '##ment',\n",
       " 'age',\n",
       " 'oh',\n",
       " 'sometimes',\n",
       " 'yes',\n",
       " 'united',\n",
       " 'bring',\n",
       " 'top',\n",
       " 'form',\n",
       " 'government',\n",
       " '##ness',\n",
       " 'built',\n",
       " 'nature',\n",
       " 'cancer',\n",
       " '##le',\n",
       " 'read',\n",
       " 'piece',\n",
       " 'research',\n",
       " 'true',\n",
       " 'beautiful',\n",
       " 'guy',\n",
       " 'until',\n",
       " 'wrong',\n",
       " 'group',\n",
       " 'words',\n",
       " 'book',\n",
       " 'heard',\n",
       " 'turn',\n",
       " 'order',\n",
       " '30',\n",
       " 'control',\n",
       " 'under',\n",
       " 'places',\n",
       " 'wasn',\n",
       " '##k',\n",
       " '93',\n",
       " 'line',\n",
       " 'species',\n",
       " '##ity',\n",
       " '91',\n",
       " 'exactly',\n",
       " 'state',\n",
       " 'internet',\n",
       " 'though',\n",
       " 'society',\n",
       " 'woman',\n",
       " 'yeah',\n",
       " 'completely',\n",
       " 'works',\n",
       " 'stop',\n",
       " '##h',\n",
       " 'art',\n",
       " 'learned',\n",
       " 'head',\n",
       " 'large',\n",
       " 'taking',\n",
       " 'became',\n",
       " 'couple',\n",
       " 'heart',\n",
       " 'happening',\n",
       " '50',\n",
       " 'decided',\n",
       " 'job',\n",
       " '##us',\n",
       " 'disease',\n",
       " 'against',\n",
       " 'level',\n",
       " 'everyone',\n",
       " 'education',\n",
       " 'itself',\n",
       " 'mother',\n",
       " 'must',\n",
       " 'run',\n",
       " 'company',\n",
       " 'friends',\n",
       " 'study',\n",
       " 'night',\n",
       " 'rather',\n",
       " 'stories',\n",
       " 'share',\n",
       " 'hear',\n",
       " 'black',\n",
       " 'kinds',\n",
       " '##an',\n",
       " 'knew',\n",
       " 'middle',\n",
       " 'model',\n",
       " 'language',\n",
       " 'questions',\n",
       " 'news',\n",
       " '##in',\n",
       " 'universe',\n",
       " 'name',\n",
       " 'animals',\n",
       " 'huge',\n",
       " '##rs',\n",
       " 'turns',\n",
       " 'gets',\n",
       " 'sound',\n",
       " 'america',\n",
       " 'somebody',\n",
       " 'themselves',\n",
       " 'century',\n",
       " 'created',\n",
       " 'word',\n",
       " 'house',\n",
       " 'india',\n",
       " 'god',\n",
       " '##ion',\n",
       " 'past',\n",
       " 'couldn',\n",
       " 'finally',\n",
       " 'ones',\n",
       " 'along',\n",
       " 'particular',\n",
       " 'perhaps',\n",
       " 'ourselves',\n",
       " 'others',\n",
       " 'front',\n",
       " 'ok',\n",
       " 'hours',\n",
       " 'lots',\n",
       " 'cell',\n",
       " 'third',\n",
       " 'air',\n",
       " 'early',\n",
       " 'worked',\n",
       " 'based',\n",
       " 'least',\n",
       " 'american',\n",
       " 'environment',\n",
       " 'students',\n",
       " '##c',\n",
       " 'outside',\n",
       " 'cities',\n",
       " '##ry',\n",
       " 'thousands',\n",
       " 'learning',\n",
       " 'systems',\n",
       " 'machine',\n",
       " 'figure',\n",
       " '##ting',\n",
       " 'per',\n",
       " 'natural',\n",
       " 'taken',\n",
       " 'china',\n",
       " 'gave',\n",
       " 'seven',\n",
       " 'ted',\n",
       " '##ive',\n",
       " 'won',\n",
       " 'free',\n",
       " 'takes',\n",
       " 'during',\n",
       " 'difficult',\n",
       " 'changed',\n",
       " 'entire',\n",
       " 'happy',\n",
       " 'companies',\n",
       " 'minutes',\n",
       " '15',\n",
       " '##p',\n",
       " 'difference',\n",
       " 'guys',\n",
       " 'ocean',\n",
       " 'area',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'death',\n",
       " 'scale',\n",
       " 'seeing',\n",
       " 'easy',\n",
       " 'close',\n",
       " 'moving',\n",
       " 'audience',\n",
       " 'culture',\n",
       " 'york',\n",
       " 'market',\n",
       " 'size',\n",
       " 'cost',\n",
       " 'turned',\n",
       " 'given',\n",
       " 'leave',\n",
       " 'population',\n",
       " 'economic',\n",
       " 'wonderful',\n",
       " 'full',\n",
       " 'image',\n",
       " 'terms',\n",
       " '##g',\n",
       " 'cannot',\n",
       " 'began',\n",
       " '##ling',\n",
       " '##ia',\n",
       " 'view',\n",
       " 'needs',\n",
       " '##ized',\n",
       " 'eyes',\n",
       " 'humans',\n",
       " 'reality',\n",
       " '##or',\n",
       " 'team',\n",
       " 'parts',\n",
       " 'parents',\n",
       " 'land',\n",
       " 'game',\n",
       " 'known',\n",
       " 'media',\n",
       " 'political',\n",
       " 'longer',\n",
       " 'simply',\n",
       " 'needed',\n",
       " '##ies',\n",
       " 'oil',\n",
       " 'whatever',\n",
       " 'hands',\n",
       " 'phone',\n",
       " 'grow',\n",
       " 'amount',\n",
       " 'local',\n",
       " 'common',\n",
       " 'white',\n",
       " 'eight',\n",
       " 'lost',\n",
       " 'powerful',\n",
       " 'yourself',\n",
       " 'father',\n",
       " 'growth',\n",
       " 'week',\n",
       " 'certain',\n",
       " 'realized',\n",
       " 'green',\n",
       " 'step',\n",
       " 'spend',\n",
       " 'born',\n",
       " 'challenge',\n",
       " 'deal',\n",
       " 'felt',\n",
       " 'interested',\n",
       " 'ability',\n",
       " 'red',\n",
       " 'center',\n",
       " '40',\n",
       " 'gone',\n",
       " 'tried',\n",
       " 'surface',\n",
       " 'test',\n",
       " 'national',\n",
       " 'walk',\n",
       " '##b',\n",
       " 'economy',\n",
       " 'girl',\n",
       " 'changes',\n",
       " 'opportunity',\n",
       " '##ist',\n",
       " 'either',\n",
       " 'field',\n",
       " '##ate',\n",
       " 'low',\n",
       " 'value',\n",
       " 'blue',\n",
       " 'scientists',\n",
       " '##th',\n",
       " 'voice',\n",
       " 'buy',\n",
       " 'program',\n",
       " 'quickly',\n",
       " 'wouldn',\n",
       " 'sitting',\n",
       " 'incredible',\n",
       " 'fish',\n",
       " 'pay',\n",
       " 'patients',\n",
       " '##ter',\n",
       " 'feet',\n",
       " 'watch',\n",
       " 'growing',\n",
       " 'dna',\n",
       " 'fear',\n",
       " 'behavior',\n",
       " 'ground',\n",
       " 'alone',\n",
       " 'technologies',\n",
       " 'weeks',\n",
       " 'developed',\n",
       " 'images',\n",
       " 'physical',\n",
       " 'structure',\n",
       " 'morning',\n",
       " 'spent',\n",
       " '##ian',\n",
       " '##less',\n",
       " 'friend',\n",
       " 'brought',\n",
       " 'climate',\n",
       " 'die',\n",
       " 'hundreds',\n",
       " 'risk',\n",
       " '##u',\n",
       " 'areas',\n",
       " 'south',\n",
       " 'street',\n",
       " 'understanding',\n",
       " '##ar',\n",
       " 'forward',\n",
       " 'literally',\n",
       " 'starting',\n",
       " 'deep',\n",
       " '##ism',\n",
       " 'material',\n",
       " 'poor',\n",
       " '##ts',\n",
       " 'complex',\n",
       " 'sea',\n",
       " 'rest',\n",
       " 'average',\n",
       " 'numbers',\n",
       " 'shows',\n",
       " 'access',\n",
       " 'short',\n",
       " '##um',\n",
       " 'met',\n",
       " 'seems',\n",
       " 'fly',\n",
       " 'animal',\n",
       " 'force',\n",
       " 'patient',\n",
       " 'books',\n",
       " 'bottom',\n",
       " 'movement',\n",
       " 'stage',\n",
       " '##en',\n",
       " 'giving',\n",
       " 'individual',\n",
       " 'law',\n",
       " 'millions',\n",
       " 'absolutely',\n",
       " 'telling',\n",
       " 'feeling',\n",
       " 'recently',\n",
       " '##ine',\n",
       " 'anyone',\n",
       " 'changing',\n",
       " 'ice',\n",
       " 'nice',\n",
       " 'speak',\n",
       " 'tools',\n",
       " 'fast',\n",
       " 'knowledge',\n",
       " 'miles',\n",
       " 'cars',\n",
       " 'kid',\n",
       " 'realize',\n",
       " 'attention',\n",
       " '##est',\n",
       " '##ize',\n",
       " 'blood',\n",
       " 'lab',\n",
       " 'map',\n",
       " 'result',\n",
       " 'key',\n",
       " '##ted',\n",
       " 'write',\n",
       " 'network',\n",
       " 'cut',\n",
       " 'film',\n",
       " '##man',\n",
       " 'paper',\n",
       " 'produce',\n",
       " 'sun',\n",
       " '##0',\n",
       " 'act',\n",
       " '##ce',\n",
       " 'clear',\n",
       " '##ble',\n",
       " 'ca',\n",
       " 'dark',\n",
       " 'eat',\n",
       " 'personal',\n",
       " '12',\n",
       " 'computers',\n",
       " 'discovered',\n",
       " 'girls',\n",
       " 'support',\n",
       " 'europe',\n",
       " 'innovation',\n",
       " '##ge',\n",
       " 'north',\n",
       " '##x',\n",
       " 'major',\n",
       " 'type',\n",
       " 'wall',\n",
       " 'industry',\n",
       " 'development',\n",
       " '##te',\n",
       " 'hold',\n",
       " 'relationship',\n",
       " 'digital',\n",
       " 'issue',\n",
       " 'lived',\n",
       " 'special',\n",
       " 'developing',\n",
       " 'solution',\n",
       " '##ch',\n",
       " 'rate',\n",
       " 'tiny',\n",
       " 'baby',\n",
       " 'gives',\n",
       " '##tion',\n",
       " 'begin',\n",
       " 'shape',\n",
       " 'allow',\n",
       " 'save',\n",
       " 'creating',\n",
       " 'medical',\n",
       " 'security',\n",
       " 'choice',\n",
       " 'fun',\n",
       " 'meet',\n",
       " '##ty',\n",
       " 'cool',\n",
       " 'effect',\n",
       " 'theory',\n",
       " 'cause',\n",
       " 'generation',\n",
       " 'running',\n",
       " 'stand',\n",
       " 'color',\n",
       " 'designed',\n",
       " 'normal',\n",
       " 'seem',\n",
       " 'solve',\n",
       " '##ent',\n",
       " 'asking',\n",
       " 'guess',\n",
       " 'please',\n",
       " 'talked',\n",
       " 'truth',\n",
       " 'putting',\n",
       " 'similar',\n",
       " 'wrote',\n",
       " 'especially',\n",
       " 'showed',\n",
       " 'soon',\n",
       " 'dead',\n",
       " 'sounds',\n",
       " '##f',\n",
       " 'becomes',\n",
       " 'groups',\n",
       " 'resources',\n",
       " 'several',\n",
       " 'box',\n",
       " 'trust',\n",
       " 'issues',\n",
       " 'nobody',\n",
       " 'term',\n",
       " 'available',\n",
       " 'modern',\n",
       " 'playing',\n",
       " 'hour',\n",
       " 'beyond',\n",
       " 'chance',\n",
       " 'experiment',\n",
       " 'likely',\n",
       " 'obviously',\n",
       " 'source',\n",
       " 'device',\n",
       " 'eye',\n",
       " 'impact',\n",
       " 'action',\n",
       " '##ally',\n",
       " 'drugs',\n",
       " 'anybody',\n",
       " 'hundred',\n",
       " '##ary',\n",
       " 'died',\n",
       " 'haven',\n",
       " 'incredibly',\n",
       " 'reasons',\n",
       " 'rules',\n",
       " 'class',\n",
       " 'towards',\n",
       " 'evolution',\n",
       " 'certainly',\n",
       " '200',\n",
       " 'movie',\n",
       " 'nine',\n",
       " 'revolution',\n",
       " 'democracy',\n",
       " 'explain',\n",
       " 'worth',\n",
       " 'stay',\n",
       " 'solar',\n",
       " 'university',\n",
       " 'drug',\n",
       " 'potential',\n",
       " 'present',\n",
       " 'code',\n",
       " 'online',\n",
       " 'pictures',\n",
       " 'violence',\n",
       " 'bigger',\n",
       " 'quality',\n",
       " 'led',\n",
       " 'product',\n",
       " ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "infectious-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "documentary-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('fr_vocab.txt', fr_vocab)\n",
    "write_vocab_file('en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "surface-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = text.BertTokenizer('fr_vocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "moral-source",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'split',\n",
       " 'split_with_offsets',\n",
       " 'submodules',\n",
       " 'tokenize',\n",
       " 'tokenize_with_offsets',\n",
       " 'trainable_variables',\n",
       " 'variables',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(en_tokenizer) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "standard-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et je ne sais pas si ça vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arrête pas de penser à mes yeux .\n",
      "Et ainsi donc , le temps étant compté , Si je vous joue littéralement juste les 2 premières lignes . C&apos; est très simple .\n",
      "Même dans les pays du monde qui ont les meilleures ressources , cet écart d&apos; espérance de vie est de 20 ans .\n",
      "\n",
      "And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .\n",
      "And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .\n",
      "Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .\n",
      "[76, 88, 12, 42, 155, 7, 78, 27, 53, 130, 114, 83, 7, 78, 27, 147, 316, 134, 87, 12, 96, 122, 42, 666, 109, 691, 497, 76, 329, 77, 1636, 12, 42, 99, 7, 78, 27, 53, 543, 336, 101, 109, 242, 691, 14]\n",
      "[76, 88, 1285, 12, 124, 149, 85, 813, 12, 114, 42, 117, 398, 83, 799, 75, 174, 297, 177, 1220, 121, 88, 14, 82, 7, 78, 27, 52, 123, 4970, 14]\n",
      "[191, 81, 75, 366, 13, 2370, 181, 395, 81, 75, 151, 12, 87, 193, 4311, 2395, 85, 103, 178, 103, 392, 161, 14]\n"
     ]
    }
   ],
   "source": [
    "for fr_examples, en_examples in train_set.batch(3).take(1):\n",
    "    for fr in fr_examples.numpy():\n",
    "        print(fr.decode('utf-8'))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n",
    "    \n",
    "    encoded = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
    "    for row in encoded.to_list():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "chinese-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[0, 102, 116, 128, 362, 121, 137, 136, 113, 333, 11, 127, 158, 116, 1543, 106, 585, 525, 120, 45, 6, 100, 26, 111, 3257, 160, 2568, 160, 11, 116, 46, 6, 100, 26, 1194, 121, 101, 423, 33, 234, 585, 13, 1], [0, 102, 288, 151, 11, 105, 202, 853, 421, 11, 137, 116, 113, 1161, 1148, 224, 106, 17, 1221, 1591, 13, 35, 6, 100, 26, 104, 153, 405, 13, 1], [0, 157, 119, 106, 233, 124, 159, 117, 144, 106, 1503, 895, 11, 260, 4122, 36, 6, 100, 26, 3950, 101, 189, 104, 101, 431, 183, 13, 1]]>\n",
      "tf.Tensor(\n",
      "[[   0  102  116  128  362  121  137  136  113  333   11  127  158  116\n",
      "  1543  106  585  525  120   45    6  100   26  111 3257  160 2568  160\n",
      "    11  116   46    6  100   26 1194  121  101  423   33  234  585   13\n",
      "     1]\n",
      " [   0  102  288  151   11  105  202  853  421   11  137  116  113 1161\n",
      "  1148  224  106   17 1221 1591   13   35    6  100   26  104  153  405\n",
      "    13    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   0  157  119  106  233  124  159  117  144  106 1503  895   11  260\n",
      "  4122   36    6  100   26 3950  101  189  104  101  431  183   13    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]], shape=(3, 43), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for fr_examples, en_examples in train_set.batch(3).take(1):\n",
    "    tokenize_pairs(fr_examples, en_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "raised-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "noticed-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "american-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "experienced-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[START]', '[END]']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reserved_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "beneficial-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "italic-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_fr_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "liable-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7836"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "distant-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 3320, 2258,  694,  940, 2560,    2,    2]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "august-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'grass', b'risks', b'reality', b'sounds', b'##ill', b'[END]', b'[END]']]>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "continued-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(fr, en):\n",
    "    fr = tokenizers.fr.tokenize(fr)\n",
    "    print(fr)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    fr = fr.to_tensor()\n",
    "    print(fr)\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return fr, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "norman-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 192304\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "anticipated-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.RaggedTensor(values=Tensor(\"StatefulPartitionedCall:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"StatefulPartitionedCall:1\", shape=(None,), dtype=int64))\n",
      "Tensor(\"RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, None), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "official-ranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et je ne sais pas si \\xc3\\xa7a vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arr\\xc3\\xaate pas de penser \\xc3\\xa0 mes yeux .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et ainsi donc , le temps \\xc3\\xa9tant compt\\xc3\\xa9 , Si je vous joue litt\\xc3\\xa9ralement juste les 2 premi\\xc3\\xa8res lignes . C&apos; est tr\\xc3\\xa8s simple .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'M\\xc3\\xaame dans les pays du monde qui ont les meilleures ressources , cet \\xc3\\xa9cart d&apos; esp\\xc3\\xa9rance de vie est de 20 ans .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .'>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "acknowledged-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  1 103 706 ...   0   0   0]\n",
      " [  1 201 443 ...   0   0   0]\n",
      " [  1 102 398 ...   0   0   0]\n",
      " ...\n",
      " [  1 102 150 ...   0   0   0]\n",
      " [  1 190  11 ...   0   0   0]\n",
      " [  1 116 204 ...   0   0   0]], shape=(64, 117), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   1  105  202 ...    0    0    0]\n",
      " [   1  241   13 ...    0    0    0]\n",
      " [   1  234 1703 ...    0    0    0]\n",
      " ...\n",
      " [   1  116   46 ...    0    0    0]\n",
      " [   1  152 1434 ...    0    0    0]\n",
      " [   1  116  174 ...    0    0    0]], shape=(64, 185), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   1   42    6 ...    0    0    0]\n",
      " [   1  102   42 ...    0    0    0]\n",
      " [   1  106 1723 ...    0    0    0]\n",
      " ...\n",
      " [   1  130 4827 ...    0    0    0]\n",
      " [   1  110  105 ...    0    0    0]\n",
      " [   1  105  150 ...    0    0    0]], shape=(64, 109), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (batch, (inp, tar)) in enumerate(train_batches.take(3)):\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "humanitarian-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "secret-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "tamil-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "controlling-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "tested-swimming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1.8, 2.2],[1.8, 2.2], [2.2, 1.8]], dtype=tf.float32)\n",
    "tf.cast(tf.shape(x)[-1], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "athletic-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0.4013123 , 0.59868765],\n",
       "       [0.4013123 , 0.59868765],\n",
       "       [0.59868765, 0.4013123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = tf.nn.softmax(x, axis=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "synthetic-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) #keep only the entire lower triangle \n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# https://www.geeksforgeeks.org/tensorflow-js-tf-linalg-bandpart-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "disciplinary-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    val = tf.matmul(q,tf.transpose(k))\n",
    "    val /= q.shape[1]\n",
    "    val = create_look_ahead_mask(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "proved-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        print(\"mask\", mask)\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        print(\"scaled_attention_logits\", scaled_attention_logits)\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "prerequisite-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "reported-sellers",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-e86f602d3b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# so the second `value` is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtemp_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-4c9d6f97ca94>\u001b[0m in \u001b[0;36mprint_out\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     temp_out, temp_attn = scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m      3\u001b[0m       q, k, v, None)\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attention weights are:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-60fc49bbc7fc>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmatmul_qk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (..., seq_len_q, seq_len_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# scale matmul_qk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3652\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3653\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3655\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5694\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5695\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5696\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5697\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5698\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-credit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
