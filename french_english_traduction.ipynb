{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-masters",
   "metadata": {},
   "source": [
    "Application of the *transformer tutorial* from https://www.tensorflow.org/text/tutorials/transformer with another dataset: french and english sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signal-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-cookie",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enhanced-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples= tfds.load('ted_multi_translate')\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mechanical-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afraid-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'ar' b'bg' b'de' b'en' b'es' b'fa' b'fr' b'gl' b'he' b'hr' b'hu' b'id'\n",
      " b'it' b'ja' b'ko' b'nl' b'pl' b'pt-br' b'ro' b'ru' b'th' b'tr' b'vi'\n",
      " b'zh-cn' b'zh-tw'], shape=(25,), dtype=string)\n",
      "1\n",
      "************************************************\n",
      "tf.Tensor([b'en'], shape=(1,), dtype=string)\n",
      "0\n",
      "************************************************\n",
      "tf.Tensor(\n",
      "[b'ar' b'bg' b'cs' b'de' b'el' b'en' b'es' b'eu' b'fa' b'fr' b'he' b'hr'\n",
      " b'hu' b'it' b'ja' b'ko' b'nl' b'pl' b'pt-br' b'ro' b'ru' b'sk' b'sq'\n",
      " b'sr' b'sv' b'th' b'tr' b'uk' b'vi' b'zh-cn' b'zh-tw'], shape=(31,), dtype=string)\n",
      "1\n",
      "************************************************\n"
     ]
    }
   ],
   "source": [
    "for item in train_examples.take(3):\n",
    "    #print(item)\n",
    "    print(item[\"translations\"][\"language\"])\n",
    "    print(len(tf.where(item[\"translations\"][\"language\"]==b\"fr\").numpy()))\n",
    "    print(\"************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enabling-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def get_sentence(df):\n",
    "    if len(tf.where(df[\"translations\"][\"language\"]==b\"fr\").numpy()):\n",
    "        lang_idx_fr = tf.where(df[\"translations\"][\"language\"]==b\"fr\").numpy()[0][0]\n",
    "        if len(tf.where(df[\"translations\"][\"language\"]==b\"en\").numpy()):\n",
    "            lang_idx_en = tf.where(df[\"translations\"][\"language\"]==b\"en\").numpy()[0][0]\n",
    "            return df['translations']['translation'][lang_idx_fr], df['translations']['translation'][lang_idx_en]\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modified-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "train_set[\"en\"]=[]\n",
    "train_set[\"fr\"]=[]\n",
    "for item in train_examples:\n",
    "    get_sentence(item)\n",
    "    if get_sentence(item) is not None:\n",
    "        train_set[\"fr\"].append(get_sentence(item)[0].numpy())\n",
    "        train_set[\"en\"].append(get_sentence(item)[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "arctic-pleasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192304"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "asian-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fr = tf.data.Dataset.from_tensor_slices(train_set[\"fr\"])\n",
    "train_en = tf.data.Dataset.from_tensor_slices(train_set[\"en\"])\n",
    "train_set = tf.data.Dataset.zip((train_fr,train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fantastic-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et je ne sais pas si \\xc3\\xa7a vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arr\\xc3\\xaate pas de penser \\xc3\\xa0 mes yeux .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et ainsi donc , le temps \\xc3\\xa9tant compt\\xc3\\xa9 , Si je vous joue litt\\xc3\\xa9ralement juste les 2 premi\\xc3\\xa8res lignes . C&apos; est tr\\xc3\\xa8s simple .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'M\\xc3\\xaame dans les pays du monde qui ont les meilleures ressources , cet \\xc3\\xa9cart d&apos; esp\\xc3\\xa9rance de vie est de 20 ans .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .'>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-artist",
   "metadata": {},
   "source": [
    "# Create french, english tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consolidated-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "partial-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "animated-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 115 ms, total: 1min 37s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_fr.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "common-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 98.6 ms, total: 1min 18s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "public-pottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " '\\\\',\n",
       " '^',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '£',\n",
       " '¿',\n",
       " 'æ',\n",
       " 'ø',\n",
       " 'τ',\n",
       " 'ย',\n",
       " 'ร',\n",
       " 'อ',\n",
       " '–',\n",
       " '—',\n",
       " '’',\n",
       " '•',\n",
       " '∇',\n",
       " '♪',\n",
       " '♫',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'apos',\n",
       " 'of',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'you',\n",
       " 'we',\n",
       " 'is',\n",
       " 'quot',\n",
       " 'this',\n",
       " 'so',\n",
       " 'they',\n",
       " 'was',\n",
       " 'for',\n",
       " '##s',\n",
       " 'are',\n",
       " 'have',\n",
       " 'what',\n",
       " 'but',\n",
       " 'on',\n",
       " 'with',\n",
       " 'can',\n",
       " 'there',\n",
       " 'about',\n",
       " 'be',\n",
       " 'as',\n",
       " 'at',\n",
       " 'all',\n",
       " 'not',\n",
       " 'do',\n",
       " 'one',\n",
       " 'my',\n",
       " 're',\n",
       " 'people',\n",
       " 'like',\n",
       " 'from',\n",
       " 'if',\n",
       " 'now',\n",
       " 'our',\n",
       " 'just',\n",
       " 'these',\n",
       " 'an',\n",
       " 'he',\n",
       " 'or',\n",
       " 'when',\n",
       " 'very',\n",
       " 'because',\n",
       " 'out',\n",
       " 'me',\n",
       " 'by',\n",
       " 'going',\n",
       " 'how',\n",
       " 'know',\n",
       " 'up',\n",
       " 'them',\n",
       " 'more',\n",
       " 'had',\n",
       " 'see',\n",
       " 'think',\n",
       " 'were',\n",
       " 'which',\n",
       " 'here',\n",
       " 'their',\n",
       " 'who',\n",
       " 'really',\n",
       " 'would',\n",
       " 'your',\n",
       " 'get',\n",
       " 'then',\n",
       " 've',\n",
       " 'us',\n",
       " 'time',\n",
       " '##ing',\n",
       " 'world',\n",
       " 'some',\n",
       " 'has',\n",
       " 'actually',\n",
       " 'don',\n",
       " 'into',\n",
       " 'where',\n",
       " 'will',\n",
       " 'way',\n",
       " '##ed',\n",
       " 'years',\n",
       " 'things',\n",
       " 'laughter',\n",
       " 'other',\n",
       " 'well',\n",
       " 'could',\n",
       " 'go',\n",
       " 'no',\n",
       " 'been',\n",
       " 'want',\n",
       " 'make',\n",
       " 'right',\n",
       " 'those',\n",
       " 'first',\n",
       " 'something',\n",
       " 'she',\n",
       " 'two',\n",
       " 'much',\n",
       " 'look',\n",
       " 'than',\n",
       " '##d',\n",
       " 'said',\n",
       " 'also',\n",
       " 'new',\n",
       " 'little',\n",
       " 'thing',\n",
       " 'got',\n",
       " 'back',\n",
       " 'over',\n",
       " 'most',\n",
       " 'even',\n",
       " 'his',\n",
       " 'life',\n",
       " 'take',\n",
       " 'only',\n",
       " 'work',\n",
       " 'say',\n",
       " '##ly',\n",
       " 'many',\n",
       " 'need',\n",
       " 'kind',\n",
       " 'did',\n",
       " 'lot',\n",
       " 'around',\n",
       " 'applause',\n",
       " 'different',\n",
       " 'why',\n",
       " 'good',\n",
       " 'every',\n",
       " 'let',\n",
       " 'down',\n",
       " 'through',\n",
       " 'her',\n",
       " 'll',\n",
       " 'same',\n",
       " 'come',\n",
       " 'being',\n",
       " 'year',\n",
       " 'three',\n",
       " 'doing',\n",
       " 'use',\n",
       " 'day',\n",
       " 'put',\n",
       " 'called',\n",
       " 'today',\n",
       " 'percent',\n",
       " 'thank',\n",
       " 'any',\n",
       " 'made',\n",
       " '##er',\n",
       " '##y',\n",
       " 'after',\n",
       " 'human',\n",
       " 'great',\n",
       " 'tell',\n",
       " 'find',\n",
       " 'fact',\n",
       " 'its',\n",
       " '##e',\n",
       " 'change',\n",
       " 'another',\n",
       " 'own',\n",
       " 'talk',\n",
       " 'didn',\n",
       " 'idea',\n",
       " 'big',\n",
       " 'last',\n",
       " 'started',\n",
       " 'before',\n",
       " 'should',\n",
       " '##a',\n",
       " '000',\n",
       " 'never',\n",
       " 'better',\n",
       " 'together',\n",
       " 'important',\n",
       " 'went',\n",
       " 'give',\n",
       " 'might',\n",
       " 'problem',\n",
       " 'thought',\n",
       " 'part',\n",
       " 'able',\n",
       " 'system',\n",
       " 'off',\n",
       " 'does',\n",
       " 'still',\n",
       " 'again',\n",
       " 'course',\n",
       " 'story',\n",
       " 'each',\n",
       " '##r',\n",
       " 'next',\n",
       " '##t',\n",
       " 'start',\n",
       " 'ago',\n",
       " 'few',\n",
       " 'long',\n",
       " 'technology',\n",
       " 'him',\n",
       " 'show',\n",
       " 'came',\n",
       " 'brain',\n",
       " 'place',\n",
       " 'bit',\n",
       " 'used',\n",
       " 'mean',\n",
       " '##n',\n",
       " 'between',\n",
       " 'old',\n",
       " 'example',\n",
       " 'water',\n",
       " '##es',\n",
       " 'too',\n",
       " 'data',\n",
       " 'question',\n",
       " 'maybe',\n",
       " 'end',\n",
       " 'looking',\n",
       " '##al',\n",
       " 'done',\n",
       " '10',\n",
       " 'found',\n",
       " 'women',\n",
       " 'doesn',\n",
       " 'point',\n",
       " 'children',\n",
       " 'understand',\n",
       " 'love',\n",
       " 'real',\n",
       " 'wanted',\n",
       " 'live',\n",
       " 'sort',\n",
       " 'four',\n",
       " 'may',\n",
       " 'ever',\n",
       " 'away',\n",
       " 'always',\n",
       " 'million',\n",
       " 'school',\n",
       " 'whole',\n",
       " 'call',\n",
       " 'trying',\n",
       " 'everything',\n",
       " 'working',\n",
       " 'person',\n",
       " 'believe',\n",
       " 'country',\n",
       " 'try',\n",
       " 'using',\n",
       " 'information',\n",
       " 'help',\n",
       " 'second',\n",
       " 'five',\n",
       " 'space',\n",
       " 'thinking',\n",
       " 'power',\n",
       " '##o',\n",
       " 'man',\n",
       " 'times',\n",
       " 'feel',\n",
       " 'high',\n",
       " 'means',\n",
       " 'number',\n",
       " 'took',\n",
       " 'small',\n",
       " 'design',\n",
       " 'create',\n",
       " 'future',\n",
       " 'kids',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'become',\n",
       " 'money',\n",
       " 'quite',\n",
       " 'building',\n",
       " 'music',\n",
       " 'city',\n",
       " 'left',\n",
       " '##ers',\n",
       " 'sense',\n",
       " 'home',\n",
       " 'without',\n",
       " 'getting',\n",
       " 'earth',\n",
       " 'best',\n",
       " 'food',\n",
       " 'comes',\n",
       " 'body',\n",
       " 'energy',\n",
       " 'happened',\n",
       " '##l',\n",
       " 'social',\n",
       " 'talking',\n",
       " 'probably',\n",
       " 'light',\n",
       " 'less',\n",
       " 'interesting',\n",
       " 'pretty',\n",
       " 'science',\n",
       " 'coming',\n",
       " 'stuff',\n",
       " 'half',\n",
       " 'am',\n",
       " 'video',\n",
       " 'imagine',\n",
       " 'across',\n",
       " 'ask',\n",
       " 'such',\n",
       " 'lives',\n",
       " '##i',\n",
       " '20',\n",
       " 'anything',\n",
       " 'told',\n",
       " 'countries',\n",
       " 'simple',\n",
       " 'dollars',\n",
       " 'play',\n",
       " 'africa',\n",
       " 'having',\n",
       " 'moment',\n",
       " 'health',\n",
       " 'happen',\n",
       " 'living',\n",
       " 'hard',\n",
       " 'saw',\n",
       " 'hand',\n",
       " 'build',\n",
       " 'ways',\n",
       " 'computer',\n",
       " 'project',\n",
       " 'cells',\n",
       " 'family',\n",
       " 'almost',\n",
       " 'okay',\n",
       " 'yet',\n",
       " 'side',\n",
       " 'makes',\n",
       " 'room',\n",
       " 'case',\n",
       " 'experience',\n",
       " 'while',\n",
       " '##ic',\n",
       " 'care',\n",
       " 'young',\n",
       " 'days',\n",
       " 'picture',\n",
       " 'later',\n",
       " 'once',\n",
       " 'far',\n",
       " 'seen',\n",
       " 'asked',\n",
       " 'process',\n",
       " 'remember',\n",
       " 'states',\n",
       " 'move',\n",
       " 'inside',\n",
       " 'nothing',\n",
       " 'goes',\n",
       " 'open',\n",
       " 'whether',\n",
       " 'billion',\n",
       " 'happens',\n",
       " 'learn',\n",
       " 'planet',\n",
       " 'basically',\n",
       " 'six',\n",
       " 'reason',\n",
       " 'says',\n",
       " 'single',\n",
       " 'both',\n",
       " 'else',\n",
       " 'global',\n",
       " 'car',\n",
       " '##able',\n",
       " 'problems',\n",
       " 'set',\n",
       " 'often',\n",
       " 'possible',\n",
       " 'bad',\n",
       " 'child',\n",
       " 'community',\n",
       " 'mind',\n",
       " 'already',\n",
       " 'within',\n",
       " 'history',\n",
       " '##m',\n",
       " 'public',\n",
       " 'ideas',\n",
       " '##on',\n",
       " 'someone',\n",
       " 'war',\n",
       " 'looked',\n",
       " 'sure',\n",
       " 'keep',\n",
       " 'men',\n",
       " 'self',\n",
       " 'everybody',\n",
       " 'hope',\n",
       " 'amazing',\n",
       " 'answer',\n",
       " 'business',\n",
       " 'looks',\n",
       " 'face',\n",
       " 'saying',\n",
       " 'matter',\n",
       " 'myself',\n",
       " '##ation',\n",
       " 'months',\n",
       " '100',\n",
       " 'instead',\n",
       " 'isn',\n",
       " 'since',\n",
       " '##ment',\n",
       " 'age',\n",
       " 'oh',\n",
       " 'sometimes',\n",
       " 'yes',\n",
       " 'united',\n",
       " 'bring',\n",
       " 'top',\n",
       " 'form',\n",
       " 'government',\n",
       " '##ness',\n",
       " 'built',\n",
       " 'nature',\n",
       " 'cancer',\n",
       " '##le',\n",
       " 'read',\n",
       " 'piece',\n",
       " 'research',\n",
       " 'true',\n",
       " 'beautiful',\n",
       " 'guy',\n",
       " 'until',\n",
       " 'wrong',\n",
       " 'group',\n",
       " 'words',\n",
       " 'book',\n",
       " 'heard',\n",
       " 'turn',\n",
       " 'order',\n",
       " '30',\n",
       " 'control',\n",
       " 'under',\n",
       " 'places',\n",
       " 'wasn',\n",
       " '##k',\n",
       " '93',\n",
       " 'line',\n",
       " 'species',\n",
       " '##ity',\n",
       " '91',\n",
       " 'exactly',\n",
       " 'state',\n",
       " 'internet',\n",
       " 'though',\n",
       " 'society',\n",
       " 'woman',\n",
       " 'yeah',\n",
       " 'completely',\n",
       " 'works',\n",
       " 'stop',\n",
       " '##h',\n",
       " 'art',\n",
       " 'learned',\n",
       " 'head',\n",
       " 'large',\n",
       " 'taking',\n",
       " 'became',\n",
       " 'couple',\n",
       " 'heart',\n",
       " 'happening',\n",
       " '50',\n",
       " 'decided',\n",
       " 'job',\n",
       " '##us',\n",
       " 'disease',\n",
       " 'against',\n",
       " 'level',\n",
       " 'everyone',\n",
       " 'education',\n",
       " 'itself',\n",
       " 'mother',\n",
       " 'must',\n",
       " 'run',\n",
       " 'company',\n",
       " 'friends',\n",
       " 'study',\n",
       " 'night',\n",
       " 'rather',\n",
       " 'stories',\n",
       " 'share',\n",
       " 'hear',\n",
       " 'black',\n",
       " 'kinds',\n",
       " '##an',\n",
       " 'knew',\n",
       " 'middle',\n",
       " 'model',\n",
       " 'language',\n",
       " 'questions',\n",
       " 'news',\n",
       " '##in',\n",
       " 'universe',\n",
       " 'name',\n",
       " 'animals',\n",
       " 'huge',\n",
       " '##rs',\n",
       " 'turns',\n",
       " 'gets',\n",
       " 'sound',\n",
       " 'america',\n",
       " 'somebody',\n",
       " 'themselves',\n",
       " 'century',\n",
       " 'created',\n",
       " 'word',\n",
       " 'house',\n",
       " 'india',\n",
       " 'god',\n",
       " '##ion',\n",
       " 'past',\n",
       " 'couldn',\n",
       " 'finally',\n",
       " 'ones',\n",
       " 'along',\n",
       " 'particular',\n",
       " 'perhaps',\n",
       " 'ourselves',\n",
       " 'others',\n",
       " 'front',\n",
       " 'ok',\n",
       " 'hours',\n",
       " 'lots',\n",
       " 'cell',\n",
       " 'third',\n",
       " 'air',\n",
       " 'early',\n",
       " 'worked',\n",
       " 'based',\n",
       " 'least',\n",
       " 'american',\n",
       " 'environment',\n",
       " 'students',\n",
       " '##c',\n",
       " 'outside',\n",
       " 'cities',\n",
       " '##ry',\n",
       " 'thousands',\n",
       " 'learning',\n",
       " 'systems',\n",
       " 'machine',\n",
       " 'figure',\n",
       " '##ting',\n",
       " 'per',\n",
       " 'natural',\n",
       " 'taken',\n",
       " 'china',\n",
       " 'gave',\n",
       " 'seven',\n",
       " 'ted',\n",
       " '##ive',\n",
       " 'won',\n",
       " 'free',\n",
       " 'takes',\n",
       " 'during',\n",
       " 'difficult',\n",
       " 'changed',\n",
       " 'entire',\n",
       " 'happy',\n",
       " 'companies',\n",
       " 'minutes',\n",
       " '15',\n",
       " '##p',\n",
       " 'difference',\n",
       " 'guys',\n",
       " 'ocean',\n",
       " 'area',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'death',\n",
       " 'scale',\n",
       " 'seeing',\n",
       " 'easy',\n",
       " 'close',\n",
       " 'moving',\n",
       " 'audience',\n",
       " 'culture',\n",
       " 'york',\n",
       " 'market',\n",
       " 'size',\n",
       " 'cost',\n",
       " 'turned',\n",
       " 'given',\n",
       " 'leave',\n",
       " 'population',\n",
       " 'economic',\n",
       " 'wonderful',\n",
       " 'full',\n",
       " 'image',\n",
       " 'terms',\n",
       " '##g',\n",
       " 'cannot',\n",
       " 'began',\n",
       " '##ling',\n",
       " '##ia',\n",
       " 'view',\n",
       " 'needs',\n",
       " '##ized',\n",
       " 'eyes',\n",
       " 'humans',\n",
       " 'reality',\n",
       " '##or',\n",
       " 'team',\n",
       " 'parts',\n",
       " 'parents',\n",
       " 'land',\n",
       " 'game',\n",
       " 'known',\n",
       " 'media',\n",
       " 'political',\n",
       " 'longer',\n",
       " 'simply',\n",
       " 'needed',\n",
       " '##ies',\n",
       " 'oil',\n",
       " 'whatever',\n",
       " 'hands',\n",
       " 'phone',\n",
       " 'grow',\n",
       " 'amount',\n",
       " 'local',\n",
       " 'common',\n",
       " 'white',\n",
       " 'eight',\n",
       " 'lost',\n",
       " 'powerful',\n",
       " 'yourself',\n",
       " 'father',\n",
       " 'growth',\n",
       " 'week',\n",
       " 'certain',\n",
       " 'realized',\n",
       " 'green',\n",
       " 'step',\n",
       " 'spend',\n",
       " 'born',\n",
       " 'challenge',\n",
       " 'deal',\n",
       " 'felt',\n",
       " 'interested',\n",
       " 'ability',\n",
       " 'red',\n",
       " 'center',\n",
       " '40',\n",
       " 'gone',\n",
       " 'tried',\n",
       " 'surface',\n",
       " 'test',\n",
       " 'national',\n",
       " 'walk',\n",
       " '##b',\n",
       " 'economy',\n",
       " 'girl',\n",
       " 'changes',\n",
       " 'opportunity',\n",
       " '##ist',\n",
       " 'either',\n",
       " 'field',\n",
       " '##ate',\n",
       " 'low',\n",
       " 'value',\n",
       " 'blue',\n",
       " 'scientists',\n",
       " '##th',\n",
       " 'voice',\n",
       " 'buy',\n",
       " 'program',\n",
       " 'quickly',\n",
       " 'wouldn',\n",
       " 'sitting',\n",
       " 'incredible',\n",
       " 'fish',\n",
       " 'pay',\n",
       " 'patients',\n",
       " '##ter',\n",
       " 'feet',\n",
       " 'watch',\n",
       " 'growing',\n",
       " 'dna',\n",
       " 'fear',\n",
       " 'behavior',\n",
       " 'ground',\n",
       " 'alone',\n",
       " 'technologies',\n",
       " 'weeks',\n",
       " 'developed',\n",
       " 'images',\n",
       " 'physical',\n",
       " 'structure',\n",
       " 'morning',\n",
       " 'spent',\n",
       " '##ian',\n",
       " '##less',\n",
       " 'friend',\n",
       " 'brought',\n",
       " 'climate',\n",
       " 'die',\n",
       " 'hundreds',\n",
       " 'risk',\n",
       " '##u',\n",
       " 'areas',\n",
       " 'south',\n",
       " 'street',\n",
       " 'understanding',\n",
       " '##ar',\n",
       " 'forward',\n",
       " 'literally',\n",
       " 'starting',\n",
       " 'deep',\n",
       " '##ism',\n",
       " 'material',\n",
       " 'poor',\n",
       " '##ts',\n",
       " 'complex',\n",
       " 'sea',\n",
       " 'rest',\n",
       " 'average',\n",
       " 'numbers',\n",
       " 'shows',\n",
       " 'access',\n",
       " 'short',\n",
       " '##um',\n",
       " 'met',\n",
       " 'seems',\n",
       " 'fly',\n",
       " 'animal',\n",
       " 'force',\n",
       " 'patient',\n",
       " 'books',\n",
       " 'bottom',\n",
       " 'movement',\n",
       " 'stage',\n",
       " '##en',\n",
       " 'giving',\n",
       " 'individual',\n",
       " 'law',\n",
       " 'millions',\n",
       " 'absolutely',\n",
       " 'telling',\n",
       " 'feeling',\n",
       " 'recently',\n",
       " '##ine',\n",
       " 'anyone',\n",
       " 'changing',\n",
       " 'ice',\n",
       " 'nice',\n",
       " 'speak',\n",
       " 'tools',\n",
       " 'fast',\n",
       " 'knowledge',\n",
       " 'miles',\n",
       " 'cars',\n",
       " 'kid',\n",
       " 'realize',\n",
       " 'attention',\n",
       " '##est',\n",
       " '##ize',\n",
       " 'blood',\n",
       " 'lab',\n",
       " 'map',\n",
       " 'result',\n",
       " 'key',\n",
       " '##ted',\n",
       " 'write',\n",
       " 'network',\n",
       " 'cut',\n",
       " 'film',\n",
       " '##man',\n",
       " 'paper',\n",
       " 'produce',\n",
       " 'sun',\n",
       " '##0',\n",
       " 'act',\n",
       " '##ce',\n",
       " 'clear',\n",
       " '##ble',\n",
       " 'ca',\n",
       " 'dark',\n",
       " 'eat',\n",
       " 'personal',\n",
       " '12',\n",
       " 'computers',\n",
       " 'discovered',\n",
       " 'girls',\n",
       " 'support',\n",
       " 'europe',\n",
       " 'innovation',\n",
       " '##ge',\n",
       " 'north',\n",
       " '##x',\n",
       " 'major',\n",
       " 'type',\n",
       " 'wall',\n",
       " 'industry',\n",
       " 'development',\n",
       " '##te',\n",
       " 'hold',\n",
       " 'relationship',\n",
       " 'digital',\n",
       " 'issue',\n",
       " 'lived',\n",
       " 'special',\n",
       " 'developing',\n",
       " 'solution',\n",
       " '##ch',\n",
       " 'rate',\n",
       " 'tiny',\n",
       " 'baby',\n",
       " 'gives',\n",
       " '##tion',\n",
       " 'begin',\n",
       " 'shape',\n",
       " 'allow',\n",
       " 'save',\n",
       " 'creating',\n",
       " 'medical',\n",
       " 'security',\n",
       " 'choice',\n",
       " 'fun',\n",
       " 'meet',\n",
       " '##ty',\n",
       " 'cool',\n",
       " 'effect',\n",
       " 'theory',\n",
       " 'cause',\n",
       " 'generation',\n",
       " 'running',\n",
       " 'stand',\n",
       " 'color',\n",
       " 'designed',\n",
       " 'normal',\n",
       " 'seem',\n",
       " 'solve',\n",
       " '##ent',\n",
       " 'asking',\n",
       " 'guess',\n",
       " 'please',\n",
       " 'talked',\n",
       " 'truth',\n",
       " 'putting',\n",
       " 'similar',\n",
       " 'wrote',\n",
       " 'especially',\n",
       " 'showed',\n",
       " 'soon',\n",
       " 'dead',\n",
       " 'sounds',\n",
       " '##f',\n",
       " 'becomes',\n",
       " 'groups',\n",
       " 'resources',\n",
       " 'several',\n",
       " 'box',\n",
       " 'trust',\n",
       " 'issues',\n",
       " 'nobody',\n",
       " 'term',\n",
       " 'available',\n",
       " 'modern',\n",
       " 'playing',\n",
       " 'hour',\n",
       " 'beyond',\n",
       " 'chance',\n",
       " 'experiment',\n",
       " 'likely',\n",
       " 'obviously',\n",
       " 'source',\n",
       " 'device',\n",
       " 'eye',\n",
       " 'impact',\n",
       " 'action',\n",
       " '##ally',\n",
       " 'drugs',\n",
       " 'anybody',\n",
       " 'hundred',\n",
       " '##ary',\n",
       " 'died',\n",
       " 'haven',\n",
       " 'incredibly',\n",
       " 'reasons',\n",
       " 'rules',\n",
       " 'class',\n",
       " 'towards',\n",
       " 'evolution',\n",
       " 'certainly',\n",
       " '200',\n",
       " 'movie',\n",
       " 'nine',\n",
       " 'revolution',\n",
       " 'democracy',\n",
       " 'explain',\n",
       " 'worth',\n",
       " 'stay',\n",
       " 'solar',\n",
       " 'university',\n",
       " 'drug',\n",
       " 'potential',\n",
       " 'present',\n",
       " 'code',\n",
       " 'online',\n",
       " 'pictures',\n",
       " 'violence',\n",
       " 'bigger',\n",
       " 'quality',\n",
       " 'led',\n",
       " 'product',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "infectious-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "documentary-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('fr_vocab.txt', fr_vocab)\n",
    "write_vocab_file('en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "surface-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = text.BertTokenizer('fr_vocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "moral-source",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'split',\n",
       " 'split_with_offsets',\n",
       " 'submodules',\n",
       " 'tokenize',\n",
       " 'tokenize_with_offsets',\n",
       " 'trainable_variables',\n",
       " 'variables',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(en_tokenizer) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "standard-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et je ne sais pas si ça vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arrête pas de penser à mes yeux .\n",
      "Et ainsi donc , le temps étant compté , Si je vous joue littéralement juste les 2 premières lignes . C&apos; est très simple .\n",
      "Même dans les pays du monde qui ont les meilleures ressources , cet écart d&apos; espérance de vie est de 20 ans .\n",
      "\n",
      "And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .\n",
      "And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .\n",
      "Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .\n",
      "[77, 89, 13, 43, 156, 8, 79, 28, 54, 131, 115, 84, 8, 79, 28, 148, 317, 135, 88, 13, 97, 123, 43, 667, 110, 692, 498, 77, 330, 78, 1637, 13, 43, 100, 8, 79, 28, 54, 544, 337, 102, 110, 243, 692, 15]\n",
      "[77, 89, 1286, 13, 125, 150, 86, 814, 13, 115, 43, 118, 399, 84, 800, 76, 175, 298, 178, 1221, 122, 89, 15, 83, 8, 79, 28, 53, 124, 4971, 15]\n",
      "[192, 82, 76, 367, 14, 2371, 182, 396, 82, 76, 152, 13, 88, 194, 4312, 2396, 86, 104, 179, 104, 393, 162, 15]\n"
     ]
    }
   ],
   "source": [
    "for fr_examples, en_examples in train_set.batch(3).take(1):\n",
    "    for fr in fr_examples.numpy():\n",
    "        print(fr.decode('utf-8'))\n",
    "\n",
    "    print()\n",
    "\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))\n",
    "    \n",
    "    encoded = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
    "    for row in encoded.to_list():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "raised-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noticed-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "american-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "experienced-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[START]', '[END]']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reserved_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beneficial-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "italic-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_fr_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "liable-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7836"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distant-indicator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 3320, 2258,  694,  940, 2560,    2,    2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "august-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'grass', b'risks', b'reality', b'sounds', b'##ill', b'[END]', b'[END]']]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "continued-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(fr, en):\n",
    "    fr = tokenizers.fr.tokenize(fr)\n",
    "    print(fr)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    fr = fr.to_tensor()\n",
    "    print(fr)\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return fr, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "blind-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 103, 117, 129, 363, 122, 138, 137, 114, 334, 12, 128, 159, 117, 1544, 107, 586, 526, 121, 46, 7, 101, 27, 112, 3258, 161, 2569, 161, 12, 117, 47, 7, 101, 27, 1195, 122, 102, 424, 34, 235, 586, 14, 2], [1, 103, 289, 152, 12, 106, 203, 854, 422, 12, 138, 117, 114, 1162, 1149, 225, 107, 18, 1222, 1592, 14, 36, 7, 101, 27, 105, 154, 406, 14, 2], [1, 158, 120, 107, 234, 125, 160, 118, 145, 107, 1504, 896, 12, 261, 4123, 37, 7, 101, 27, 3951, 102, 190, 105, 102, 432, 184, 14, 2]]>\n",
      "tf.Tensor(\n",
      "[[   1  103  117  129  363  122  138  137  114  334   12  128  159  117\n",
      "  1544  107  586  526  121   46    7  101   27  112 3258  161 2569  161\n",
      "    12  117   47    7  101   27 1195  122  102  424   34  235  586   14\n",
      "     2]\n",
      " [   1  103  289  152   12  106  203  854  422   12  138  117  114 1162\n",
      "  1149  225  107   18 1222 1592   14   36    7  101   27  105  154  406\n",
      "    14    2    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   1  158  120  107  234  125  160  118  145  107 1504  896   12  261\n",
      "  4123   37    7  101   27 3951  102  190  105  102  432  184   14    2\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]], shape=(3, 43), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for fr_examples, en_examples in train_set.batch(3).take(1):\n",
    "    tokenize_pairs(fr_examples, en_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "norman-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 192304\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "anticipated-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.RaggedTensor(values=Tensor(\"StatefulPartitionedCall:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"StatefulPartitionedCall:1\", shape=(None,), dtype=int64))\n",
      "Tensor(\"RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, None), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "official-ranking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et je ne sais pas si \\xc3\\xa7a vous arrive , mais quand je ferme les yeux parfois pour m&apos; endormir , Je n&apos; arr\\xc3\\xaate pas de penser \\xc3\\xa0 mes yeux .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so , I don &apos;t know if you &apos;ve ever had this , but when I close my eyes sometimes and try to sleep , I can &apos;t stop thinking about my own eyes .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Et ainsi donc , le temps \\xc3\\xa9tant compt\\xc3\\xa9 , Si je vous joue litt\\xc3\\xa9ralement juste les 2 premi\\xc3\\xa8res lignes . C&apos; est tr\\xc3\\xa8s simple .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'And so therefore , because time is short , if I just play you literally the first maybe two lines or so . It &apos;s very straightforward .'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'M\\xc3\\xaame dans les pays du monde qui ont les meilleures ressources , cet \\xc3\\xa9cart d&apos; esp\\xc3\\xa9rance de vie est de 20 ans .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'Even in the best-resourced countries in the world , this life expectancy gap is as much as 20 years .'>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_set.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acknowledged-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  1 103 202 ...   0   0   0]\n",
      " [  1 106 620 ...   0   0   0]\n",
      " [  1 112 457 ...   0   0   0]\n",
      " ...\n",
      " [  1 167 548 ...   0   0   0]\n",
      " [  1  36   7 ...   0   0   0]\n",
      " [  1 103 176 ...   0   0   0]], shape=(64, 98), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   1   36    7 ...    0    0    0]\n",
      " [   1  103  137 ...    0    0    0]\n",
      " [   1  667   18 ...    0    0    0]\n",
      " ...\n",
      " [   1  116   58 ...    0    0    0]\n",
      " [   1  103  116 ...    0    0    0]\n",
      " [   1  111 6141 ...    0    0    0]], shape=(64, 139), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[   1  106 6762 ...    0    0    0]\n",
      " [   1  117 3625 ...    0    0    0]\n",
      " [   1  128  285 ...    0    0    0]\n",
      " ...\n",
      " [   1  132  116 ...    0    0    0]\n",
      " [   1  349  137 ...    0    0    0]\n",
      " [   1  156  128 ...    0    0    0]], shape=(64, 147), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for (batch, (inp, tar)) in enumerate(train_batches.take(3)):\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-bruce",
   "metadata": {},
   "source": [
    "# Building transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-thong",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "humanitarian-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "secret-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-luxembourg",
   "metadata": {},
   "source": [
    "## scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tamil-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "controlling-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "tested-swimming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1.8, 2.2],[1.8, 2.2], [2.2, 1.8]], dtype=tf.float32)\n",
    "tf.cast(tf.shape(x)[-1], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "athletic-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0.4013123 , 0.59868765],\n",
       "       [0.4013123 , 0.59868765],\n",
       "       [0.59868765, 0.4013123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = tf.nn.softmax(x, axis=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "synthetic-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) #keep only the entire lower triangle \n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# https://www.geeksforgeeks.org/tensorflow-js-tf-linalg-bandpart-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "proved-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        #print(\"mask\", mask)\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        #print(\"scaled_attention_logits\", scaled_attention_logits)\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "prerequisite-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "reported-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-egyptian",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dying-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "        #For each batch, we have self.num_heads attentions, and for each attention a subspace of representation \n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "reliable-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "inside-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "angry-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        self.mha_1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha_2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.encoder = EncoderLayer(d_model, num_heads, dff, rate)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self,x, training, mask_1, mask_enc):\n",
    "        output_mha_1 = self.mha_1(x, x,x, mask_1)\n",
    "        output_mha_1 = self.dropout1(output_mha_1, training=training)\n",
    "        output_norm_1 = self.layernorm1(x + output_mha_1) \n",
    "        \n",
    "        output_encoder = self.encoder(x, training, mask_enc)\n",
    "        \n",
    "        output_mha_2 = self.mha_1(output_encoder, output_encoder,output_norm_1, mask_2)\n",
    "        output_mha_2 = self.dropout1(output_mha_2, training=training)\n",
    "        output_norm_2 = self.layernorm1(output_mha_2 + output_norm_1) \n",
    "        \n",
    "\n",
    "        \n",
    "        ffn_output = self.ffn(output_norm_2)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(output_norm_2 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "coordinated-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "economic-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        #    print(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #print(tf.math.sqrt(tf.cast(self.d_model, tf.float32)))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "documented-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "mysterious-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        #print(dec_padding_mask)\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-missile",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "romantic-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aerial-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "circular-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "everyday-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "matched-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "caroline-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "separate-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "molecular-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "inner-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "internal-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp],\n",
    "                                     training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "small-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.3823 Accuracy 0.7062\n",
      "Epoch 1 Batch 50 Loss 1.4987 Accuracy 0.6864\n",
      "Epoch 1 Batch 100 Loss 1.4989 Accuracy 0.6850\n",
      "Epoch 1 Batch 150 Loss 1.4974 Accuracy 0.6857\n",
      "Epoch 1 Batch 200 Loss 1.4970 Accuracy 0.6854\n",
      "Epoch 1 Batch 250 Loss 1.4966 Accuracy 0.6854\n",
      "Epoch 1 Batch 300 Loss 1.4984 Accuracy 0.6855\n",
      "Epoch 1 Batch 350 Loss 1.5004 Accuracy 0.6850\n",
      "Epoch 1 Batch 400 Loss 1.5032 Accuracy 0.6848\n",
      "Epoch 1 Batch 450 Loss 1.5021 Accuracy 0.6848\n",
      "Epoch 1 Batch 500 Loss 1.5034 Accuracy 0.6848\n",
      "Epoch 1 Batch 550 Loss 1.5072 Accuracy 0.6841\n",
      "Epoch 1 Batch 600 Loss 1.5076 Accuracy 0.6841\n",
      "Epoch 1 Batch 650 Loss 1.5089 Accuracy 0.6839\n",
      "Epoch 1 Batch 700 Loss 1.5106 Accuracy 0.6837\n",
      "Epoch 1 Batch 750 Loss 1.5112 Accuracy 0.6837\n",
      "Epoch 1 Batch 800 Loss 1.5137 Accuracy 0.6833\n",
      "Epoch 1 Batch 850 Loss 1.5146 Accuracy 0.6830\n",
      "Epoch 1 Batch 900 Loss 1.5155 Accuracy 0.6828\n",
      "Epoch 1 Batch 950 Loss 1.5161 Accuracy 0.6826\n",
      "Epoch 1 Batch 1000 Loss 1.5166 Accuracy 0.6825\n",
      "Epoch 1 Batch 1050 Loss 1.5169 Accuracy 0.6826\n",
      "Epoch 1 Batch 1100 Loss 1.5182 Accuracy 0.6825\n",
      "Epoch 1 Batch 1150 Loss 1.5201 Accuracy 0.6823\n",
      "Epoch 1 Batch 1200 Loss 1.5196 Accuracy 0.6823\n",
      "Epoch 1 Batch 1250 Loss 1.5211 Accuracy 0.6821\n",
      "Epoch 1 Batch 1300 Loss 1.5224 Accuracy 0.6818\n",
      "Epoch 1 Batch 1350 Loss 1.5229 Accuracy 0.6818\n",
      "Epoch 1 Batch 1400 Loss 1.5237 Accuracy 0.6817\n",
      "Epoch 1 Batch 1450 Loss 1.5248 Accuracy 0.6816\n",
      "Epoch 1 Batch 1500 Loss 1.5253 Accuracy 0.6816\n",
      "Epoch 1 Batch 1550 Loss 1.5255 Accuracy 0.6816\n",
      "Epoch 1 Batch 1600 Loss 1.5255 Accuracy 0.6816\n",
      "Epoch 1 Batch 1650 Loss 1.5268 Accuracy 0.6814\n",
      "Epoch 1 Batch 1700 Loss 1.5268 Accuracy 0.6815\n",
      "Epoch 1 Batch 1750 Loss 1.5277 Accuracy 0.6813\n",
      "Epoch 1 Batch 1800 Loss 1.5281 Accuracy 0.6813\n",
      "Epoch 1 Batch 1850 Loss 1.5282 Accuracy 0.6813\n",
      "Epoch 1 Batch 1900 Loss 1.5290 Accuracy 0.6812\n",
      "Epoch 1 Batch 1950 Loss 1.5295 Accuracy 0.6811\n",
      "Epoch 1 Batch 2000 Loss 1.5301 Accuracy 0.6810\n",
      "Epoch 1 Batch 2050 Loss 1.5308 Accuracy 0.6809\n",
      "Epoch 1 Batch 2100 Loss 1.5322 Accuracy 0.6807\n",
      "Epoch 1 Batch 2150 Loss 1.5331 Accuracy 0.6806\n",
      "Epoch 1 Batch 2200 Loss 1.5338 Accuracy 0.6805\n",
      "Epoch 1 Batch 2250 Loss 1.5345 Accuracy 0.6804\n",
      "Epoch 1 Batch 2300 Loss 1.5347 Accuracy 0.6805\n",
      "Epoch 1 Batch 2350 Loss 1.5352 Accuracy 0.6804\n",
      "Epoch 1 Batch 2400 Loss 1.5358 Accuracy 0.6803\n",
      "Epoch 1 Batch 2450 Loss 1.5362 Accuracy 0.6803\n",
      "Epoch 1 Batch 2500 Loss 1.5365 Accuracy 0.6802\n",
      "Epoch 1 Batch 2550 Loss 1.5370 Accuracy 0.6801\n",
      "Epoch 1 Batch 2600 Loss 1.5377 Accuracy 0.6800\n",
      "Epoch 1 Batch 2650 Loss 1.5379 Accuracy 0.6799\n",
      "Epoch 1 Batch 2700 Loss 1.5389 Accuracy 0.6798\n",
      "Epoch 1 Batch 2750 Loss 1.5395 Accuracy 0.6797\n",
      "Epoch 1 Batch 2800 Loss 1.5402 Accuracy 0.6796\n",
      "Epoch 1 Batch 2850 Loss 1.5408 Accuracy 0.6795\n",
      "Epoch 1 Batch 2900 Loss 1.5410 Accuracy 0.6795\n",
      "Epoch 1 Batch 2950 Loss 1.5413 Accuracy 0.6795\n",
      "Epoch 1 Batch 3000 Loss 1.5417 Accuracy 0.6795\n",
      "Epoch 1 Loss 1.5418 Accuracy 0.6794\n",
      "Time taken for 1 epoch: 248.70 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4743 Accuracy 0.6871\n",
      "Epoch 2 Batch 50 Loss 1.4918 Accuracy 0.6870\n",
      "Epoch 2 Batch 100 Loss 1.4953 Accuracy 0.6850\n",
      "Epoch 2 Batch 150 Loss 1.4912 Accuracy 0.6856\n",
      "Epoch 2 Batch 200 Loss 1.4856 Accuracy 0.6870\n",
      "Epoch 2 Batch 250 Loss 1.4846 Accuracy 0.6869\n",
      "Epoch 2 Batch 300 Loss 1.4899 Accuracy 0.6861\n",
      "Epoch 2 Batch 350 Loss 1.4874 Accuracy 0.6868\n",
      "Epoch 2 Batch 400 Loss 1.4891 Accuracy 0.6865\n",
      "Epoch 2 Batch 450 Loss 1.4897 Accuracy 0.6864\n",
      "Epoch 2 Batch 500 Loss 1.4925 Accuracy 0.6857\n",
      "Epoch 2 Batch 550 Loss 1.4938 Accuracy 0.6856\n",
      "Epoch 2 Batch 600 Loss 1.4944 Accuracy 0.6856\n",
      "Epoch 2 Batch 650 Loss 1.4974 Accuracy 0.6851\n",
      "Epoch 2 Batch 700 Loss 1.4978 Accuracy 0.6850\n",
      "Epoch 2 Batch 750 Loss 1.4993 Accuracy 0.6848\n",
      "Epoch 2 Batch 800 Loss 1.5008 Accuracy 0.6846\n",
      "Epoch 2 Batch 850 Loss 1.5011 Accuracy 0.6846\n",
      "Epoch 2 Batch 900 Loss 1.5023 Accuracy 0.6844\n",
      "Epoch 2 Batch 950 Loss 1.5038 Accuracy 0.6841\n",
      "Epoch 2 Batch 1000 Loss 1.5039 Accuracy 0.6841\n",
      "Epoch 2 Batch 1050 Loss 1.5047 Accuracy 0.6840\n",
      "Epoch 2 Batch 1100 Loss 1.5060 Accuracy 0.6838\n",
      "Epoch 2 Batch 1150 Loss 1.5070 Accuracy 0.6837\n",
      "Epoch 2 Batch 1200 Loss 1.5078 Accuracy 0.6835\n",
      "Epoch 2 Batch 1250 Loss 1.5080 Accuracy 0.6835\n",
      "Epoch 2 Batch 1300 Loss 1.5085 Accuracy 0.6835\n",
      "Epoch 2 Batch 1350 Loss 1.5099 Accuracy 0.6833\n",
      "Epoch 2 Batch 1400 Loss 1.5105 Accuracy 0.6833\n",
      "Epoch 2 Batch 1450 Loss 1.5115 Accuracy 0.6832\n",
      "Epoch 2 Batch 1500 Loss 1.5132 Accuracy 0.6829\n",
      "Epoch 2 Batch 1550 Loss 1.5139 Accuracy 0.6829\n",
      "Epoch 2 Batch 1600 Loss 1.5147 Accuracy 0.6828\n",
      "Epoch 2 Batch 1650 Loss 1.5155 Accuracy 0.6827\n",
      "Epoch 2 Batch 1700 Loss 1.5162 Accuracy 0.6827\n",
      "Epoch 2 Batch 1750 Loss 1.5172 Accuracy 0.6825\n",
      "Epoch 2 Batch 1800 Loss 1.5178 Accuracy 0.6824\n",
      "Epoch 2 Batch 1850 Loss 1.5188 Accuracy 0.6823\n",
      "Epoch 2 Batch 1900 Loss 1.5199 Accuracy 0.6822\n",
      "Epoch 2 Batch 1950 Loss 1.5206 Accuracy 0.6821\n",
      "Epoch 2 Batch 2000 Loss 1.5213 Accuracy 0.6819\n",
      "Epoch 2 Batch 2050 Loss 1.5211 Accuracy 0.6820\n",
      "Epoch 2 Batch 2100 Loss 1.5220 Accuracy 0.6819\n",
      "Epoch 2 Batch 2150 Loss 1.5231 Accuracy 0.6817\n",
      "Epoch 2 Batch 2200 Loss 1.5241 Accuracy 0.6816\n",
      "Epoch 2 Batch 2250 Loss 1.5245 Accuracy 0.6816\n",
      "Epoch 2 Batch 2300 Loss 1.5249 Accuracy 0.6815\n",
      "Epoch 2 Batch 2350 Loss 1.5252 Accuracy 0.6815\n",
      "Epoch 2 Batch 2400 Loss 1.5260 Accuracy 0.6814\n",
      "Epoch 2 Batch 2450 Loss 1.5262 Accuracy 0.6814\n",
      "Epoch 2 Batch 2500 Loss 1.5271 Accuracy 0.6813\n",
      "Epoch 2 Batch 2550 Loss 1.5275 Accuracy 0.6813\n",
      "Epoch 2 Batch 2600 Loss 1.5279 Accuracy 0.6813\n",
      "Epoch 2 Batch 2650 Loss 1.5279 Accuracy 0.6812\n",
      "Epoch 2 Batch 2700 Loss 1.5286 Accuracy 0.6811\n",
      "Epoch 2 Batch 2750 Loss 1.5290 Accuracy 0.6811\n",
      "Epoch 2 Batch 2800 Loss 1.5300 Accuracy 0.6810\n",
      "Epoch 2 Batch 2850 Loss 1.5308 Accuracy 0.6809\n",
      "Epoch 2 Batch 2900 Loss 1.5311 Accuracy 0.6808\n",
      "Epoch 2 Batch 2950 Loss 1.5317 Accuracy 0.6807\n",
      "Epoch 2 Batch 3000 Loss 1.5326 Accuracy 0.6806\n",
      "Epoch 2 Loss 1.5326 Accuracy 0.6806\n",
      "Time taken for 1 epoch: 238.79 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6158 Accuracy 0.6629\n",
      "Epoch 3 Batch 50 Loss 1.4733 Accuracy 0.6885\n",
      "Epoch 3 Batch 100 Loss 1.4773 Accuracy 0.6885\n",
      "Epoch 3 Batch 150 Loss 1.4646 Accuracy 0.6904\n",
      "Epoch 3 Batch 200 Loss 1.4731 Accuracy 0.6893\n",
      "Epoch 3 Batch 250 Loss 1.4790 Accuracy 0.6886\n",
      "Epoch 3 Batch 300 Loss 1.4817 Accuracy 0.6879\n",
      "Epoch 3 Batch 350 Loss 1.4853 Accuracy 0.6874\n",
      "Epoch 3 Batch 400 Loss 1.4888 Accuracy 0.6867\n",
      "Epoch 3 Batch 450 Loss 1.4917 Accuracy 0.6862\n",
      "Epoch 3 Batch 500 Loss 1.4931 Accuracy 0.6859\n",
      "Epoch 3 Batch 550 Loss 1.4937 Accuracy 0.6859\n",
      "Epoch 3 Batch 600 Loss 1.4974 Accuracy 0.6854\n",
      "Epoch 3 Batch 650 Loss 1.4984 Accuracy 0.6852\n",
      "Epoch 3 Batch 700 Loss 1.5002 Accuracy 0.6850\n",
      "Epoch 3 Batch 750 Loss 1.5007 Accuracy 0.6850\n",
      "Epoch 3 Batch 800 Loss 1.5000 Accuracy 0.6852\n",
      "Epoch 3 Batch 850 Loss 1.5011 Accuracy 0.6850\n",
      "Epoch 3 Batch 900 Loss 1.5001 Accuracy 0.6852\n",
      "Epoch 3 Batch 950 Loss 1.5019 Accuracy 0.6849\n",
      "Epoch 3 Batch 1000 Loss 1.5017 Accuracy 0.6850\n",
      "Epoch 3 Batch 1050 Loss 1.5026 Accuracy 0.6848\n",
      "Epoch 3 Batch 1100 Loss 1.5032 Accuracy 0.6848\n",
      "Epoch 3 Batch 1150 Loss 1.5021 Accuracy 0.6850\n",
      "Epoch 3 Batch 1200 Loss 1.5023 Accuracy 0.6850\n",
      "Epoch 3 Batch 1250 Loss 1.5030 Accuracy 0.6849\n",
      "Epoch 3 Batch 1300 Loss 1.5046 Accuracy 0.6847\n",
      "Epoch 3 Batch 1350 Loss 1.5061 Accuracy 0.6845\n",
      "Epoch 3 Batch 1400 Loss 1.5064 Accuracy 0.6845\n",
      "Epoch 3 Batch 1450 Loss 1.5070 Accuracy 0.6844\n",
      "Epoch 3 Batch 1500 Loss 1.5085 Accuracy 0.6842\n",
      "Epoch 3 Batch 1550 Loss 1.5094 Accuracy 0.6841\n",
      "Epoch 3 Batch 1600 Loss 1.5100 Accuracy 0.6840\n",
      "Epoch 3 Batch 1650 Loss 1.5105 Accuracy 0.6838\n",
      "Epoch 3 Batch 1700 Loss 1.5106 Accuracy 0.6839\n",
      "Epoch 3 Batch 1750 Loss 1.5113 Accuracy 0.6838\n",
      "Epoch 3 Batch 1800 Loss 1.5116 Accuracy 0.6838\n",
      "Epoch 3 Batch 1850 Loss 1.5121 Accuracy 0.6837\n",
      "Epoch 3 Batch 1900 Loss 1.5126 Accuracy 0.6837\n",
      "Epoch 3 Batch 1950 Loss 1.5132 Accuracy 0.6836\n",
      "Epoch 3 Batch 2000 Loss 1.5142 Accuracy 0.6834\n",
      "Epoch 3 Batch 2050 Loss 1.5145 Accuracy 0.6834\n",
      "Epoch 3 Batch 2100 Loss 1.5152 Accuracy 0.6833\n",
      "Epoch 3 Batch 2150 Loss 1.5152 Accuracy 0.6833\n",
      "Epoch 3 Batch 2200 Loss 1.5157 Accuracy 0.6833\n",
      "Epoch 3 Batch 2250 Loss 1.5164 Accuracy 0.6832\n",
      "Epoch 3 Batch 2300 Loss 1.5170 Accuracy 0.6831\n",
      "Epoch 3 Batch 2350 Loss 1.5168 Accuracy 0.6831\n",
      "Epoch 3 Batch 2400 Loss 1.5175 Accuracy 0.6830\n",
      "Epoch 3 Batch 2450 Loss 1.5181 Accuracy 0.6829\n",
      "Epoch 3 Batch 2500 Loss 1.5180 Accuracy 0.6830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 2550 Loss 1.5193 Accuracy 0.6828\n",
      "Epoch 3 Batch 2600 Loss 1.5196 Accuracy 0.6828\n",
      "Epoch 3 Batch 2650 Loss 1.5204 Accuracy 0.6827\n",
      "Epoch 3 Batch 2700 Loss 1.5207 Accuracy 0.6826\n",
      "Epoch 3 Batch 2750 Loss 1.5213 Accuracy 0.6826\n",
      "Epoch 3 Batch 2800 Loss 1.5221 Accuracy 0.6825\n",
      "Epoch 3 Batch 2850 Loss 1.5226 Accuracy 0.6824\n",
      "Epoch 3 Batch 2900 Loss 1.5229 Accuracy 0.6824\n",
      "Epoch 3 Batch 2950 Loss 1.5235 Accuracy 0.6823\n",
      "Epoch 3 Batch 3000 Loss 1.5236 Accuracy 0.6823\n",
      "Epoch 3 Loss 1.5238 Accuracy 0.6823\n",
      "Time taken for 1 epoch: 237.70 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5437 Accuracy 0.6754\n",
      "Epoch 4 Batch 50 Loss 1.4609 Accuracy 0.6908\n",
      "Epoch 4 Batch 100 Loss 1.4715 Accuracy 0.6889\n",
      "Epoch 4 Batch 150 Loss 1.4663 Accuracy 0.6900\n",
      "Epoch 4 Batch 200 Loss 1.4668 Accuracy 0.6899\n",
      "Epoch 4 Batch 250 Loss 1.4695 Accuracy 0.6891\n",
      "Epoch 4 Batch 300 Loss 1.4692 Accuracy 0.6893\n",
      "Epoch 4 Batch 350 Loss 1.4735 Accuracy 0.6889\n",
      "Epoch 4 Batch 400 Loss 1.4741 Accuracy 0.6888\n",
      "Epoch 4 Batch 450 Loss 1.4743 Accuracy 0.6888\n",
      "Epoch 4 Batch 500 Loss 1.4752 Accuracy 0.6885\n",
      "Epoch 4 Batch 550 Loss 1.4776 Accuracy 0.6882\n",
      "Epoch 4 Batch 600 Loss 1.4802 Accuracy 0.6880\n",
      "Epoch 4 Batch 650 Loss 1.4822 Accuracy 0.6878\n",
      "Epoch 4 Batch 700 Loss 1.4829 Accuracy 0.6876\n",
      "Epoch 4 Batch 750 Loss 1.4845 Accuracy 0.6873\n",
      "Epoch 4 Batch 800 Loss 1.4867 Accuracy 0.6869\n",
      "Epoch 4 Batch 850 Loss 1.4878 Accuracy 0.6867\n",
      "Epoch 4 Batch 900 Loss 1.4881 Accuracy 0.6868\n",
      "Epoch 4 Batch 950 Loss 1.4890 Accuracy 0.6867\n",
      "Epoch 4 Batch 1000 Loss 1.4893 Accuracy 0.6867\n",
      "Epoch 4 Batch 1050 Loss 1.4909 Accuracy 0.6865\n",
      "Epoch 4 Batch 1100 Loss 1.4933 Accuracy 0.6861\n",
      "Epoch 4 Batch 1150 Loss 1.4939 Accuracy 0.6861\n",
      "Epoch 4 Batch 1200 Loss 1.4941 Accuracy 0.6861\n",
      "Epoch 4 Batch 1250 Loss 1.4949 Accuracy 0.6860\n",
      "Epoch 4 Batch 1300 Loss 1.4962 Accuracy 0.6858\n",
      "Epoch 4 Batch 1350 Loss 1.4969 Accuracy 0.6857\n",
      "Epoch 4 Batch 1400 Loss 1.4975 Accuracy 0.6857\n",
      "Epoch 4 Batch 1450 Loss 1.4981 Accuracy 0.6856\n",
      "Epoch 4 Batch 1500 Loss 1.4983 Accuracy 0.6857\n",
      "Epoch 4 Batch 1550 Loss 1.4991 Accuracy 0.6856\n",
      "Epoch 4 Batch 1600 Loss 1.5002 Accuracy 0.6854\n",
      "Epoch 4 Batch 1650 Loss 1.5010 Accuracy 0.6852\n",
      "Epoch 4 Batch 1700 Loss 1.5020 Accuracy 0.6851\n",
      "Epoch 4 Batch 1750 Loss 1.5013 Accuracy 0.6852\n",
      "Epoch 4 Batch 1800 Loss 1.5020 Accuracy 0.6851\n",
      "Epoch 4 Batch 1850 Loss 1.5029 Accuracy 0.6850\n",
      "Epoch 4 Batch 1900 Loss 1.5042 Accuracy 0.6848\n",
      "Epoch 4 Batch 1950 Loss 1.5048 Accuracy 0.6848\n",
      "Epoch 4 Batch 2000 Loss 1.5053 Accuracy 0.6847\n",
      "Epoch 4 Batch 2050 Loss 1.5063 Accuracy 0.6845\n",
      "Epoch 4 Batch 2100 Loss 1.5069 Accuracy 0.6844\n",
      "Epoch 4 Batch 2150 Loss 1.5074 Accuracy 0.6843\n",
      "Epoch 4 Batch 2200 Loss 1.5081 Accuracy 0.6842\n",
      "Epoch 4 Batch 2250 Loss 1.5084 Accuracy 0.6842\n",
      "Epoch 4 Batch 2300 Loss 1.5089 Accuracy 0.6842\n",
      "Epoch 4 Batch 2350 Loss 1.5093 Accuracy 0.6841\n",
      "Epoch 4 Batch 2400 Loss 1.5101 Accuracy 0.6840\n",
      "Epoch 4 Batch 2450 Loss 1.5106 Accuracy 0.6839\n",
      "Epoch 4 Batch 2500 Loss 1.5110 Accuracy 0.6838\n",
      "Epoch 4 Batch 2550 Loss 1.5115 Accuracy 0.6838\n",
      "Epoch 4 Batch 2600 Loss 1.5120 Accuracy 0.6837\n",
      "Epoch 4 Batch 2650 Loss 1.5122 Accuracy 0.6837\n",
      "Epoch 4 Batch 2700 Loss 1.5128 Accuracy 0.6837\n",
      "Epoch 4 Batch 2750 Loss 1.5131 Accuracy 0.6836\n",
      "Epoch 4 Batch 2800 Loss 1.5134 Accuracy 0.6836\n",
      "Epoch 4 Batch 2850 Loss 1.5138 Accuracy 0.6836\n",
      "Epoch 4 Batch 2900 Loss 1.5144 Accuracy 0.6835\n",
      "Epoch 4 Batch 2950 Loss 1.5150 Accuracy 0.6834\n",
      "Epoch 4 Batch 3000 Loss 1.5156 Accuracy 0.6834\n",
      "Epoch 4 Loss 1.5158 Accuracy 0.6833\n",
      "Time taken for 1 epoch: 240.33 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5816 Accuracy 0.6708\n",
      "Epoch 5 Batch 50 Loss 1.4597 Accuracy 0.6911\n",
      "Epoch 5 Batch 100 Loss 1.4684 Accuracy 0.6901\n",
      "Epoch 5 Batch 150 Loss 1.4622 Accuracy 0.6913\n",
      "Epoch 5 Batch 200 Loss 1.4633 Accuracy 0.6910\n",
      "Epoch 5 Batch 250 Loss 1.4594 Accuracy 0.6918\n",
      "Epoch 5 Batch 300 Loss 1.4649 Accuracy 0.6910\n",
      "Epoch 5 Batch 350 Loss 1.4664 Accuracy 0.6907\n",
      "Epoch 5 Batch 400 Loss 1.4701 Accuracy 0.6903\n",
      "Epoch 5 Batch 450 Loss 1.4724 Accuracy 0.6901\n",
      "Epoch 5 Batch 500 Loss 1.4723 Accuracy 0.6900\n",
      "Epoch 5 Batch 550 Loss 1.4725 Accuracy 0.6900\n",
      "Epoch 5 Batch 600 Loss 1.4734 Accuracy 0.6898\n",
      "Epoch 5 Batch 650 Loss 1.4746 Accuracy 0.6895\n",
      "Epoch 5 Batch 700 Loss 1.4747 Accuracy 0.6895\n",
      "Epoch 5 Batch 750 Loss 1.4745 Accuracy 0.6895\n",
      "Epoch 5 Batch 800 Loss 1.4754 Accuracy 0.6893\n",
      "Epoch 5 Batch 850 Loss 1.4764 Accuracy 0.6891\n",
      "Epoch 5 Batch 900 Loss 1.4776 Accuracy 0.6890\n",
      "Epoch 5 Batch 950 Loss 1.4790 Accuracy 0.6887\n",
      "Epoch 5 Batch 1000 Loss 1.4797 Accuracy 0.6886\n",
      "Epoch 5 Batch 1050 Loss 1.4808 Accuracy 0.6885\n",
      "Epoch 5 Batch 1100 Loss 1.4832 Accuracy 0.6881\n",
      "Epoch 5 Batch 1150 Loss 1.4836 Accuracy 0.6880\n",
      "Epoch 5 Batch 1200 Loss 1.4840 Accuracy 0.6880\n",
      "Epoch 5 Batch 1250 Loss 1.4849 Accuracy 0.6879\n",
      "Epoch 5 Batch 1300 Loss 1.4851 Accuracy 0.6879\n",
      "Epoch 5 Batch 1350 Loss 1.4862 Accuracy 0.6877\n",
      "Epoch 5 Batch 1400 Loss 1.4868 Accuracy 0.6877\n",
      "Epoch 5 Batch 1450 Loss 1.4874 Accuracy 0.6876\n",
      "Epoch 5 Batch 1500 Loss 1.4884 Accuracy 0.6875\n",
      "Epoch 5 Batch 1550 Loss 1.4897 Accuracy 0.6872\n",
      "Epoch 5 Batch 1600 Loss 1.4905 Accuracy 0.6871\n",
      "Epoch 5 Batch 1650 Loss 1.4917 Accuracy 0.6870\n",
      "Epoch 5 Batch 1700 Loss 1.4927 Accuracy 0.6869\n",
      "Epoch 5 Batch 1750 Loss 1.4933 Accuracy 0.6867\n",
      "Epoch 5 Batch 1800 Loss 1.4943 Accuracy 0.6866\n",
      "Epoch 5 Batch 1850 Loss 1.4944 Accuracy 0.6866\n",
      "Epoch 5 Batch 1900 Loss 1.4950 Accuracy 0.6865\n",
      "Epoch 5 Batch 1950 Loss 1.4953 Accuracy 0.6865\n",
      "Epoch 5 Batch 2000 Loss 1.4959 Accuracy 0.6863\n",
      "Epoch 5 Batch 2050 Loss 1.4971 Accuracy 0.6862\n",
      "Epoch 5 Batch 2100 Loss 1.4979 Accuracy 0.6860\n",
      "Epoch 5 Batch 2150 Loss 1.4984 Accuracy 0.6859\n",
      "Epoch 5 Batch 2200 Loss 1.4991 Accuracy 0.6858\n",
      "Epoch 5 Batch 2250 Loss 1.4995 Accuracy 0.6858\n",
      "Epoch 5 Batch 2300 Loss 1.5001 Accuracy 0.6857\n",
      "Epoch 5 Batch 2350 Loss 1.5005 Accuracy 0.6856\n",
      "Epoch 5 Batch 2400 Loss 1.5006 Accuracy 0.6857\n",
      "Epoch 5 Batch 2450 Loss 1.5013 Accuracy 0.6856\n",
      "Epoch 5 Batch 2500 Loss 1.5016 Accuracy 0.6856\n",
      "Epoch 5 Batch 2550 Loss 1.5020 Accuracy 0.6855\n",
      "Epoch 5 Batch 2600 Loss 1.5027 Accuracy 0.6854\n",
      "Epoch 5 Batch 2650 Loss 1.5036 Accuracy 0.6853\n",
      "Epoch 5 Batch 2700 Loss 1.5040 Accuracy 0.6853\n",
      "Epoch 5 Batch 2750 Loss 1.5047 Accuracy 0.6851\n",
      "Epoch 5 Batch 2800 Loss 1.5050 Accuracy 0.6851\n",
      "Epoch 5 Batch 2850 Loss 1.5054 Accuracy 0.6850\n",
      "Epoch 5 Batch 2900 Loss 1.5062 Accuracy 0.6849\n",
      "Epoch 5 Batch 2950 Loss 1.5069 Accuracy 0.6848\n",
      "Epoch 5 Batch 3000 Loss 1.5074 Accuracy 0.6848\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
      "Epoch 5 Loss 1.5074 Accuracy 0.6848\n",
      "Time taken for 1 epoch: 239.47 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.3592 Accuracy 0.7188\n",
      "Epoch 6 Batch 50 Loss 1.4523 Accuracy 0.6944\n",
      "Epoch 6 Batch 100 Loss 1.4514 Accuracy 0.6941\n",
      "Epoch 6 Batch 150 Loss 1.4501 Accuracy 0.6941\n",
      "Epoch 6 Batch 200 Loss 1.4547 Accuracy 0.6928\n",
      "Epoch 6 Batch 250 Loss 1.4597 Accuracy 0.6920\n",
      "Epoch 6 Batch 300 Loss 1.4607 Accuracy 0.6920\n",
      "Epoch 6 Batch 350 Loss 1.4587 Accuracy 0.6920\n",
      "Epoch 6 Batch 400 Loss 1.4610 Accuracy 0.6917\n",
      "Epoch 6 Batch 450 Loss 1.4625 Accuracy 0.6914\n",
      "Epoch 6 Batch 500 Loss 1.4656 Accuracy 0.6909\n",
      "Epoch 6 Batch 550 Loss 1.4690 Accuracy 0.6901\n",
      "Epoch 6 Batch 600 Loss 1.4705 Accuracy 0.6901\n",
      "Epoch 6 Batch 650 Loss 1.4722 Accuracy 0.6898\n",
      "Epoch 6 Batch 700 Loss 1.4739 Accuracy 0.6894\n",
      "Epoch 6 Batch 750 Loss 1.4757 Accuracy 0.6892\n",
      "Epoch 6 Batch 800 Loss 1.4769 Accuracy 0.6889\n",
      "Epoch 6 Batch 850 Loss 1.4772 Accuracy 0.6889\n",
      "Epoch 6 Batch 900 Loss 1.4783 Accuracy 0.6888\n",
      "Epoch 6 Batch 950 Loss 1.4788 Accuracy 0.6887\n",
      "Epoch 6 Batch 1000 Loss 1.4799 Accuracy 0.6884\n",
      "Epoch 6 Batch 1050 Loss 1.4809 Accuracy 0.6883\n",
      "Epoch 6 Batch 1100 Loss 1.4812 Accuracy 0.6883\n",
      "Epoch 6 Batch 1150 Loss 1.4816 Accuracy 0.6882\n",
      "Epoch 6 Batch 1200 Loss 1.4820 Accuracy 0.6881\n",
      "Epoch 6 Batch 1250 Loss 1.4818 Accuracy 0.6881\n",
      "Epoch 6 Batch 1300 Loss 1.4824 Accuracy 0.6881\n",
      "Epoch 6 Batch 1350 Loss 1.4832 Accuracy 0.6879\n",
      "Epoch 6 Batch 1400 Loss 1.4831 Accuracy 0.6879\n",
      "Epoch 6 Batch 1450 Loss 1.4840 Accuracy 0.6880\n",
      "Epoch 6 Batch 1500 Loss 1.4847 Accuracy 0.6878\n",
      "Epoch 6 Batch 1550 Loss 1.4860 Accuracy 0.6877\n",
      "Epoch 6 Batch 1600 Loss 1.4866 Accuracy 0.6876\n",
      "Epoch 6 Batch 1650 Loss 1.4871 Accuracy 0.6875\n",
      "Epoch 6 Batch 1700 Loss 1.4878 Accuracy 0.6874\n",
      "Epoch 6 Batch 1750 Loss 1.4884 Accuracy 0.6873\n",
      "Epoch 6 Batch 1800 Loss 1.4890 Accuracy 0.6873\n",
      "Epoch 6 Batch 1850 Loss 1.4898 Accuracy 0.6871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 1900 Loss 1.4906 Accuracy 0.6871\n",
      "Epoch 6 Batch 1950 Loss 1.4909 Accuracy 0.6871\n",
      "Epoch 6 Batch 2000 Loss 1.4910 Accuracy 0.6870\n",
      "Epoch 6 Batch 2050 Loss 1.4919 Accuracy 0.6869\n",
      "Epoch 6 Batch 2100 Loss 1.4919 Accuracy 0.6870\n",
      "Epoch 6 Batch 2150 Loss 1.4926 Accuracy 0.6868\n",
      "Epoch 6 Batch 2200 Loss 1.4930 Accuracy 0.6867\n",
      "Epoch 6 Batch 2250 Loss 1.4936 Accuracy 0.6867\n",
      "Epoch 6 Batch 2300 Loss 1.4938 Accuracy 0.6866\n",
      "Epoch 6 Batch 2350 Loss 1.4948 Accuracy 0.6865\n",
      "Epoch 6 Batch 2400 Loss 1.4955 Accuracy 0.6864\n",
      "Epoch 6 Batch 2450 Loss 1.4962 Accuracy 0.6863\n",
      "Epoch 6 Batch 2500 Loss 1.4969 Accuracy 0.6862\n",
      "Epoch 6 Batch 2550 Loss 1.4977 Accuracy 0.6861\n",
      "Epoch 6 Batch 2600 Loss 1.4983 Accuracy 0.6860\n",
      "Epoch 6 Batch 2650 Loss 1.4984 Accuracy 0.6861\n",
      "Epoch 6 Batch 2700 Loss 1.4988 Accuracy 0.6861\n",
      "Epoch 6 Batch 2750 Loss 1.4989 Accuracy 0.6861\n",
      "Epoch 6 Batch 2800 Loss 1.4991 Accuracy 0.6861\n",
      "Epoch 6 Batch 2850 Loss 1.4995 Accuracy 0.6860\n",
      "Epoch 6 Batch 2900 Loss 1.5000 Accuracy 0.6859\n",
      "Epoch 6 Batch 2950 Loss 1.5001 Accuracy 0.6859\n",
      "Epoch 6 Batch 3000 Loss 1.5007 Accuracy 0.6858\n",
      "Epoch 6 Loss 1.5009 Accuracy 0.6858\n",
      "Time taken for 1 epoch: 244.08 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.4682 Accuracy 0.6905\n",
      "Epoch 7 Batch 50 Loss 1.4597 Accuracy 0.6897\n",
      "Epoch 7 Batch 100 Loss 1.4566 Accuracy 0.6913\n",
      "Epoch 7 Batch 150 Loss 1.4494 Accuracy 0.6934\n",
      "Epoch 7 Batch 200 Loss 1.4503 Accuracy 0.6934\n",
      "Epoch 7 Batch 250 Loss 1.4486 Accuracy 0.6931\n",
      "Epoch 7 Batch 300 Loss 1.4518 Accuracy 0.6927\n",
      "Epoch 7 Batch 350 Loss 1.4484 Accuracy 0.6933\n",
      "Epoch 7 Batch 400 Loss 1.4515 Accuracy 0.6929\n",
      "Epoch 7 Batch 450 Loss 1.4512 Accuracy 0.6929\n",
      "Epoch 7 Batch 500 Loss 1.4516 Accuracy 0.6927\n",
      "Epoch 7 Batch 550 Loss 1.4508 Accuracy 0.6929\n",
      "Epoch 7 Batch 600 Loss 1.4531 Accuracy 0.6924\n",
      "Epoch 7 Batch 650 Loss 1.4548 Accuracy 0.6922\n",
      "Epoch 7 Batch 700 Loss 1.4545 Accuracy 0.6923\n",
      "Epoch 7 Batch 750 Loss 1.4567 Accuracy 0.6919\n",
      "Epoch 7 Batch 800 Loss 1.4593 Accuracy 0.6915\n",
      "Epoch 7 Batch 850 Loss 1.4608 Accuracy 0.6913\n",
      "Epoch 7 Batch 900 Loss 1.4623 Accuracy 0.6910\n",
      "Epoch 7 Batch 950 Loss 1.4636 Accuracy 0.6908\n",
      "Epoch 7 Batch 1000 Loss 1.4641 Accuracy 0.6908\n",
      "Epoch 7 Batch 1050 Loss 1.4654 Accuracy 0.6907\n",
      "Epoch 7 Batch 1100 Loss 1.4667 Accuracy 0.6905\n",
      "Epoch 7 Batch 1150 Loss 1.4674 Accuracy 0.6905\n",
      "Epoch 7 Batch 1200 Loss 1.4684 Accuracy 0.6903\n",
      "Epoch 7 Batch 1250 Loss 1.4687 Accuracy 0.6903\n",
      "Epoch 7 Batch 1300 Loss 1.4697 Accuracy 0.6902\n",
      "Epoch 7 Batch 1350 Loss 1.4704 Accuracy 0.6902\n",
      "Epoch 7 Batch 1400 Loss 1.4710 Accuracy 0.6901\n",
      "Epoch 7 Batch 1450 Loss 1.4723 Accuracy 0.6899\n",
      "Epoch 7 Batch 1500 Loss 1.4733 Accuracy 0.6898\n",
      "Epoch 7 Batch 1550 Loss 1.4744 Accuracy 0.6897\n",
      "Epoch 7 Batch 1600 Loss 1.4754 Accuracy 0.6895\n",
      "Epoch 7 Batch 1650 Loss 1.4759 Accuracy 0.6895\n",
      "Epoch 7 Batch 1700 Loss 1.4766 Accuracy 0.6894\n",
      "Epoch 7 Batch 1750 Loss 1.4780 Accuracy 0.6891\n",
      "Epoch 7 Batch 1800 Loss 1.4789 Accuracy 0.6890\n",
      "Epoch 7 Batch 1850 Loss 1.4800 Accuracy 0.6889\n",
      "Epoch 7 Batch 1900 Loss 1.4805 Accuracy 0.6888\n",
      "Epoch 7 Batch 1950 Loss 1.4814 Accuracy 0.6886\n",
      "Epoch 7 Batch 2000 Loss 1.4819 Accuracy 0.6885\n",
      "Epoch 7 Batch 2050 Loss 1.4823 Accuracy 0.6884\n",
      "Epoch 7 Batch 2100 Loss 1.4828 Accuracy 0.6884\n",
      "Epoch 7 Batch 2150 Loss 1.4836 Accuracy 0.6883\n",
      "Epoch 7 Batch 2200 Loss 1.4836 Accuracy 0.6884\n",
      "Epoch 7 Batch 2250 Loss 1.4845 Accuracy 0.6882\n",
      "Epoch 7 Batch 2300 Loss 1.4848 Accuracy 0.6882\n",
      "Epoch 7 Batch 2350 Loss 1.4856 Accuracy 0.6880\n",
      "Epoch 7 Batch 2400 Loss 1.4857 Accuracy 0.6880\n",
      "Epoch 7 Batch 2450 Loss 1.4862 Accuracy 0.6880\n",
      "Epoch 7 Batch 2500 Loss 1.4869 Accuracy 0.6879\n",
      "Epoch 7 Batch 2550 Loss 1.4874 Accuracy 0.6879\n",
      "Epoch 7 Batch 2600 Loss 1.4884 Accuracy 0.6878\n",
      "Epoch 7 Batch 2650 Loss 1.4894 Accuracy 0.6876\n",
      "Epoch 7 Batch 2700 Loss 1.4902 Accuracy 0.6875\n",
      "Epoch 7 Batch 2750 Loss 1.4907 Accuracy 0.6874\n",
      "Epoch 7 Batch 2800 Loss 1.4918 Accuracy 0.6872\n",
      "Epoch 7 Batch 2850 Loss 1.4923 Accuracy 0.6871\n",
      "Epoch 7 Batch 2900 Loss 1.4927 Accuracy 0.6870\n",
      "Epoch 7 Batch 2950 Loss 1.4931 Accuracy 0.6870\n",
      "Epoch 7 Batch 3000 Loss 1.4936 Accuracy 0.6869\n",
      "Epoch 7 Loss 1.4936 Accuracy 0.6869\n",
      "Time taken for 1 epoch: 243.86 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.5269 Accuracy 0.6690\n",
      "Epoch 8 Batch 50 Loss 1.4292 Accuracy 0.6954\n",
      "Epoch 8 Batch 100 Loss 1.4319 Accuracy 0.6948\n",
      "Epoch 8 Batch 150 Loss 1.4304 Accuracy 0.6951\n",
      "Epoch 8 Batch 200 Loss 1.4307 Accuracy 0.6956\n",
      "Epoch 8 Batch 250 Loss 1.4342 Accuracy 0.6953\n",
      "Epoch 8 Batch 300 Loss 1.4411 Accuracy 0.6945\n",
      "Epoch 8 Batch 350 Loss 1.4449 Accuracy 0.6939\n",
      "Epoch 8 Batch 400 Loss 1.4445 Accuracy 0.6938\n",
      "Epoch 8 Batch 450 Loss 1.4455 Accuracy 0.6935\n",
      "Epoch 8 Batch 500 Loss 1.4474 Accuracy 0.6931\n",
      "Epoch 8 Batch 550 Loss 1.4479 Accuracy 0.6930\n",
      "Epoch 8 Batch 600 Loss 1.4506 Accuracy 0.6924\n",
      "Epoch 8 Batch 650 Loss 1.4555 Accuracy 0.6917\n",
      "Epoch 8 Batch 700 Loss 1.4567 Accuracy 0.6916\n",
      "Epoch 8 Batch 750 Loss 1.4580 Accuracy 0.6914\n",
      "Epoch 8 Batch 800 Loss 1.4595 Accuracy 0.6912\n",
      "Epoch 8 Batch 850 Loss 1.4600 Accuracy 0.6911\n",
      "Epoch 8 Batch 900 Loss 1.4608 Accuracy 0.6909\n",
      "Epoch 8 Batch 950 Loss 1.4627 Accuracy 0.6907\n",
      "Epoch 8 Batch 1000 Loss 1.4636 Accuracy 0.6907\n",
      "Epoch 8 Batch 1050 Loss 1.4651 Accuracy 0.6905\n",
      "Epoch 8 Batch 1100 Loss 1.4675 Accuracy 0.6901\n",
      "Epoch 8 Batch 1150 Loss 1.4681 Accuracy 0.6901\n",
      "Epoch 8 Batch 1200 Loss 1.4682 Accuracy 0.6902\n",
      "Epoch 8 Batch 1250 Loss 1.4694 Accuracy 0.6900\n",
      "Epoch 8 Batch 1300 Loss 1.4688 Accuracy 0.6901\n",
      "Epoch 8 Batch 1350 Loss 1.4686 Accuracy 0.6903\n",
      "Epoch 8 Batch 1400 Loss 1.4693 Accuracy 0.6902\n",
      "Epoch 8 Batch 1450 Loss 1.4698 Accuracy 0.6901\n",
      "Epoch 8 Batch 1500 Loss 1.4704 Accuracy 0.6901\n",
      "Epoch 8 Batch 1550 Loss 1.4714 Accuracy 0.6900\n",
      "Epoch 8 Batch 1600 Loss 1.4731 Accuracy 0.6897\n",
      "Epoch 8 Batch 1650 Loss 1.4728 Accuracy 0.6897\n",
      "Epoch 8 Batch 1700 Loss 1.4737 Accuracy 0.6896\n",
      "Epoch 8 Batch 1750 Loss 1.4743 Accuracy 0.6896\n",
      "Epoch 8 Batch 1800 Loss 1.4748 Accuracy 0.6896\n",
      "Epoch 8 Batch 1850 Loss 1.4751 Accuracy 0.6895\n",
      "Epoch 8 Batch 1900 Loss 1.4757 Accuracy 0.6895\n",
      "Epoch 8 Batch 1950 Loss 1.4759 Accuracy 0.6894\n",
      "Epoch 8 Batch 2000 Loss 1.4764 Accuracy 0.6893\n",
      "Epoch 8 Batch 2050 Loss 1.4766 Accuracy 0.6894\n",
      "Epoch 8 Batch 2100 Loss 1.4770 Accuracy 0.6894\n",
      "Epoch 8 Batch 2150 Loss 1.4777 Accuracy 0.6892\n",
      "Epoch 8 Batch 2200 Loss 1.4779 Accuracy 0.6892\n",
      "Epoch 8 Batch 2250 Loss 1.4785 Accuracy 0.6891\n",
      "Epoch 8 Batch 2300 Loss 1.4792 Accuracy 0.6891\n",
      "Epoch 8 Batch 2350 Loss 1.4799 Accuracy 0.6890\n",
      "Epoch 8 Batch 2400 Loss 1.4800 Accuracy 0.6889\n",
      "Epoch 8 Batch 2450 Loss 1.4811 Accuracy 0.6888\n",
      "Epoch 8 Batch 2500 Loss 1.4814 Accuracy 0.6887\n",
      "Epoch 8 Batch 2550 Loss 1.4822 Accuracy 0.6886\n",
      "Epoch 8 Batch 2600 Loss 1.4830 Accuracy 0.6884\n",
      "Epoch 8 Batch 2650 Loss 1.4836 Accuracy 0.6884\n",
      "Epoch 8 Batch 2700 Loss 1.4836 Accuracy 0.6884\n",
      "Epoch 8 Batch 2750 Loss 1.4843 Accuracy 0.6883\n",
      "Epoch 8 Batch 2800 Loss 1.4850 Accuracy 0.6882\n",
      "Epoch 8 Batch 2850 Loss 1.4854 Accuracy 0.6882\n",
      "Epoch 8 Batch 2900 Loss 1.4863 Accuracy 0.6880\n",
      "Epoch 8 Batch 2950 Loss 1.4871 Accuracy 0.6879\n",
      "Epoch 8 Batch 3000 Loss 1.4873 Accuracy 0.6879\n",
      "Epoch 8 Loss 1.4875 Accuracy 0.6879\n",
      "Time taken for 1 epoch: 240.68 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.6579 Accuracy 0.6671\n",
      "Epoch 9 Batch 50 Loss 1.4403 Accuracy 0.6938\n",
      "Epoch 9 Batch 100 Loss 1.4281 Accuracy 0.6963\n",
      "Epoch 9 Batch 150 Loss 1.4313 Accuracy 0.6956\n",
      "Epoch 9 Batch 200 Loss 1.4312 Accuracy 0.6955\n",
      "Epoch 9 Batch 250 Loss 1.4350 Accuracy 0.6949\n",
      "Epoch 9 Batch 300 Loss 1.4388 Accuracy 0.6941\n",
      "Epoch 9 Batch 350 Loss 1.4415 Accuracy 0.6938\n",
      "Epoch 9 Batch 400 Loss 1.4422 Accuracy 0.6939\n",
      "Epoch 9 Batch 450 Loss 1.4441 Accuracy 0.6937\n",
      "Epoch 9 Batch 500 Loss 1.4449 Accuracy 0.6937\n",
      "Epoch 9 Batch 550 Loss 1.4452 Accuracy 0.6937\n",
      "Epoch 9 Batch 600 Loss 1.4489 Accuracy 0.6931\n",
      "Epoch 9 Batch 650 Loss 1.4516 Accuracy 0.6927\n",
      "Epoch 9 Batch 700 Loss 1.4525 Accuracy 0.6926\n",
      "Epoch 9 Batch 750 Loss 1.4550 Accuracy 0.6922\n",
      "Epoch 9 Batch 800 Loss 1.4550 Accuracy 0.6923\n",
      "Epoch 9 Batch 850 Loss 1.4560 Accuracy 0.6921\n",
      "Epoch 9 Batch 900 Loss 1.4570 Accuracy 0.6919\n",
      "Epoch 9 Batch 950 Loss 1.4572 Accuracy 0.6920\n",
      "Epoch 9 Batch 1000 Loss 1.4596 Accuracy 0.6916\n",
      "Epoch 9 Batch 1050 Loss 1.4608 Accuracy 0.6915\n",
      "Epoch 9 Batch 1100 Loss 1.4618 Accuracy 0.6913\n",
      "Epoch 9 Batch 1150 Loss 1.4630 Accuracy 0.6911\n",
      "Epoch 9 Batch 1200 Loss 1.4637 Accuracy 0.6911\n",
      "Epoch 9 Batch 1250 Loss 1.4645 Accuracy 0.6909\n",
      "Epoch 9 Batch 1300 Loss 1.4653 Accuracy 0.6909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 1350 Loss 1.4660 Accuracy 0.6909\n",
      "Epoch 9 Batch 1400 Loss 1.4670 Accuracy 0.6907\n",
      "Epoch 9 Batch 1450 Loss 1.4668 Accuracy 0.6908\n",
      "Epoch 9 Batch 1500 Loss 1.4677 Accuracy 0.6907\n",
      "Epoch 9 Batch 1550 Loss 1.4679 Accuracy 0.6907\n",
      "Epoch 9 Batch 1600 Loss 1.4689 Accuracy 0.6905\n",
      "Epoch 9 Batch 1650 Loss 1.4699 Accuracy 0.6903\n",
      "Epoch 9 Batch 1700 Loss 1.4700 Accuracy 0.6903\n",
      "Epoch 9 Batch 1750 Loss 1.4704 Accuracy 0.6902\n",
      "Epoch 9 Batch 1800 Loss 1.4715 Accuracy 0.6901\n",
      "Epoch 9 Batch 1850 Loss 1.4719 Accuracy 0.6900\n",
      "Epoch 9 Batch 1900 Loss 1.4733 Accuracy 0.6898\n",
      "Epoch 9 Batch 1950 Loss 1.4739 Accuracy 0.6897\n",
      "Epoch 9 Batch 2000 Loss 1.4744 Accuracy 0.6897\n",
      "Epoch 9 Batch 2050 Loss 1.4745 Accuracy 0.6896\n",
      "Epoch 9 Batch 2100 Loss 1.4749 Accuracy 0.6896\n",
      "Epoch 9 Batch 2150 Loss 1.4752 Accuracy 0.6896\n",
      "Epoch 9 Batch 2200 Loss 1.4757 Accuracy 0.6895\n",
      "Epoch 9 Batch 2250 Loss 1.4761 Accuracy 0.6895\n",
      "Epoch 9 Batch 2300 Loss 1.4762 Accuracy 0.6895\n",
      "Epoch 9 Batch 2350 Loss 1.4764 Accuracy 0.6894\n",
      "Epoch 9 Batch 2400 Loss 1.4769 Accuracy 0.6894\n",
      "Epoch 9 Batch 2450 Loss 1.4777 Accuracy 0.6893\n",
      "Epoch 9 Batch 2500 Loss 1.4776 Accuracy 0.6893\n",
      "Epoch 9 Batch 2550 Loss 1.4785 Accuracy 0.6892\n",
      "Epoch 9 Batch 2600 Loss 1.4792 Accuracy 0.6891\n",
      "Epoch 9 Batch 2650 Loss 1.4796 Accuracy 0.6890\n",
      "Epoch 9 Batch 2700 Loss 1.4793 Accuracy 0.6891\n",
      "Epoch 9 Batch 2750 Loss 1.4800 Accuracy 0.6890\n",
      "Epoch 9 Batch 2800 Loss 1.4802 Accuracy 0.6890\n",
      "Epoch 9 Batch 2850 Loss 1.4801 Accuracy 0.6890\n",
      "Epoch 9 Batch 2900 Loss 1.4811 Accuracy 0.6889\n",
      "Epoch 9 Batch 2950 Loss 1.4817 Accuracy 0.6888\n",
      "Epoch 9 Batch 3000 Loss 1.4822 Accuracy 0.6888\n",
      "Epoch 9 Loss 1.4822 Accuracy 0.6888\n",
      "Time taken for 1 epoch: 239.84 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.4335 Accuracy 0.7009\n",
      "Epoch 10 Batch 50 Loss 1.4196 Accuracy 0.6968\n",
      "Epoch 10 Batch 100 Loss 1.4289 Accuracy 0.6952\n",
      "Epoch 10 Batch 150 Loss 1.4315 Accuracy 0.6950\n",
      "Epoch 10 Batch 200 Loss 1.4384 Accuracy 0.6938\n",
      "Epoch 10 Batch 250 Loss 1.4410 Accuracy 0.6935\n",
      "Epoch 10 Batch 300 Loss 1.4380 Accuracy 0.6944\n",
      "Epoch 10 Batch 350 Loss 1.4392 Accuracy 0.6943\n",
      "Epoch 10 Batch 400 Loss 1.4399 Accuracy 0.6942\n",
      "Epoch 10 Batch 450 Loss 1.4416 Accuracy 0.6938\n",
      "Epoch 10 Batch 500 Loss 1.4431 Accuracy 0.6935\n",
      "Epoch 10 Batch 550 Loss 1.4451 Accuracy 0.6933\n",
      "Epoch 10 Batch 600 Loss 1.4472 Accuracy 0.6932\n",
      "Epoch 10 Batch 650 Loss 1.4493 Accuracy 0.6930\n",
      "Epoch 10 Batch 700 Loss 1.4498 Accuracy 0.6929\n",
      "Epoch 10 Batch 750 Loss 1.4502 Accuracy 0.6930\n",
      "Epoch 10 Batch 800 Loss 1.4510 Accuracy 0.6930\n",
      "Epoch 10 Batch 850 Loss 1.4521 Accuracy 0.6928\n",
      "Epoch 10 Batch 900 Loss 1.4529 Accuracy 0.6927\n",
      "Epoch 10 Batch 950 Loss 1.4537 Accuracy 0.6927\n",
      "Epoch 10 Batch 1000 Loss 1.4528 Accuracy 0.6928\n",
      "Epoch 10 Batch 1050 Loss 1.4548 Accuracy 0.6924\n",
      "Epoch 10 Batch 1100 Loss 1.4551 Accuracy 0.6923\n",
      "Epoch 10 Batch 1150 Loss 1.4550 Accuracy 0.6923\n",
      "Epoch 10 Batch 1200 Loss 1.4550 Accuracy 0.6923\n",
      "Epoch 10 Batch 1250 Loss 1.4563 Accuracy 0.6921\n",
      "Epoch 10 Batch 1300 Loss 1.4568 Accuracy 0.6922\n",
      "Epoch 10 Batch 1350 Loss 1.4576 Accuracy 0.6921\n",
      "Epoch 10 Batch 1400 Loss 1.4590 Accuracy 0.6920\n",
      "Epoch 10 Batch 1450 Loss 1.4595 Accuracy 0.6919\n",
      "Epoch 10 Batch 1500 Loss 1.4607 Accuracy 0.6917\n",
      "Epoch 10 Batch 1550 Loss 1.4621 Accuracy 0.6915\n",
      "Epoch 10 Batch 1600 Loss 1.4622 Accuracy 0.6915\n",
      "Epoch 10 Batch 1650 Loss 1.4636 Accuracy 0.6913\n",
      "Epoch 10 Batch 1700 Loss 1.4641 Accuracy 0.6912\n",
      "Epoch 10 Batch 1750 Loss 1.4642 Accuracy 0.6912\n",
      "Epoch 10 Batch 1800 Loss 1.4648 Accuracy 0.6912\n",
      "Epoch 10 Batch 1850 Loss 1.4649 Accuracy 0.6912\n",
      "Epoch 10 Batch 1900 Loss 1.4651 Accuracy 0.6912\n",
      "Epoch 10 Batch 1950 Loss 1.4655 Accuracy 0.6912\n",
      "Epoch 10 Batch 2000 Loss 1.4662 Accuracy 0.6911\n",
      "Epoch 10 Batch 2050 Loss 1.4673 Accuracy 0.6909\n",
      "Epoch 10 Batch 2100 Loss 1.4680 Accuracy 0.6908\n",
      "Epoch 10 Batch 2150 Loss 1.4681 Accuracy 0.6908\n",
      "Epoch 10 Batch 2200 Loss 1.4687 Accuracy 0.6908\n",
      "Epoch 10 Batch 2250 Loss 1.4691 Accuracy 0.6907\n",
      "Epoch 10 Batch 2300 Loss 1.4695 Accuracy 0.6906\n",
      "Epoch 10 Batch 2350 Loss 1.4698 Accuracy 0.6906\n",
      "Epoch 10 Batch 2400 Loss 1.4702 Accuracy 0.6906\n",
      "Epoch 10 Batch 2450 Loss 1.4706 Accuracy 0.6906\n",
      "Epoch 10 Batch 2500 Loss 1.4706 Accuracy 0.6906\n",
      "Epoch 10 Batch 2550 Loss 1.4713 Accuracy 0.6905\n",
      "Epoch 10 Batch 2600 Loss 1.4717 Accuracy 0.6904\n",
      "Epoch 10 Batch 2650 Loss 1.4723 Accuracy 0.6903\n",
      "Epoch 10 Batch 2700 Loss 1.4728 Accuracy 0.6902\n",
      "Epoch 10 Batch 2750 Loss 1.4734 Accuracy 0.6901\n",
      "Epoch 10 Batch 2800 Loss 1.4737 Accuracy 0.6902\n",
      "Epoch 10 Batch 2850 Loss 1.4740 Accuracy 0.6901\n",
      "Epoch 10 Batch 2900 Loss 1.4745 Accuracy 0.6901\n",
      "Epoch 10 Batch 2950 Loss 1.4749 Accuracy 0.6901\n",
      "Epoch 10 Batch 3000 Loss 1.4751 Accuracy 0.6901\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
      "Epoch 10 Loss 1.4752 Accuracy 0.6900\n",
      "Time taken for 1 epoch: 240.80 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.6106 Accuracy 0.6641\n",
      "Epoch 11 Batch 50 Loss 1.4250 Accuracy 0.6962\n",
      "Epoch 11 Batch 100 Loss 1.4336 Accuracy 0.6951\n",
      "Epoch 11 Batch 150 Loss 1.4407 Accuracy 0.6941\n",
      "Epoch 11 Batch 200 Loss 1.4387 Accuracy 0.6946\n",
      "Epoch 11 Batch 250 Loss 1.4436 Accuracy 0.6938\n",
      "Epoch 11 Batch 300 Loss 1.4450 Accuracy 0.6936\n",
      "Epoch 11 Batch 350 Loss 1.4460 Accuracy 0.6936\n",
      "Epoch 11 Batch 400 Loss 1.4443 Accuracy 0.6937\n",
      "Epoch 11 Batch 450 Loss 1.4428 Accuracy 0.6939\n",
      "Epoch 11 Batch 500 Loss 1.4399 Accuracy 0.6945\n",
      "Epoch 11 Batch 550 Loss 1.4412 Accuracy 0.6944\n",
      "Epoch 11 Batch 600 Loss 1.4436 Accuracy 0.6941\n",
      "Epoch 11 Batch 650 Loss 1.4445 Accuracy 0.6939\n",
      "Epoch 11 Batch 700 Loss 1.4462 Accuracy 0.6935\n",
      "Epoch 11 Batch 750 Loss 1.4476 Accuracy 0.6933\n",
      "Epoch 11 Batch 800 Loss 1.4490 Accuracy 0.6931\n",
      "Epoch 11 Batch 850 Loss 1.4491 Accuracy 0.6932\n",
      "Epoch 11 Batch 900 Loss 1.4499 Accuracy 0.6931\n",
      "Epoch 11 Batch 950 Loss 1.4514 Accuracy 0.6929\n",
      "Epoch 11 Batch 1000 Loss 1.4522 Accuracy 0.6928\n",
      "Epoch 11 Batch 1050 Loss 1.4522 Accuracy 0.6927\n",
      "Epoch 11 Batch 1100 Loss 1.4519 Accuracy 0.6928\n",
      "Epoch 11 Batch 1150 Loss 1.4530 Accuracy 0.6927\n",
      "Epoch 11 Batch 1200 Loss 1.4537 Accuracy 0.6926\n",
      "Epoch 11 Batch 1250 Loss 1.4532 Accuracy 0.6926\n",
      "Epoch 11 Batch 1300 Loss 1.4540 Accuracy 0.6925\n",
      "Epoch 11 Batch 1350 Loss 1.4541 Accuracy 0.6926\n",
      "Epoch 11 Batch 1400 Loss 1.4542 Accuracy 0.6926\n",
      "Epoch 11 Batch 1450 Loss 1.4556 Accuracy 0.6923\n",
      "Epoch 11 Batch 1500 Loss 1.4564 Accuracy 0.6922\n",
      "Epoch 11 Batch 1550 Loss 1.4578 Accuracy 0.6921\n",
      "Epoch 11 Batch 1600 Loss 1.4588 Accuracy 0.6920\n",
      "Epoch 11 Batch 1650 Loss 1.4594 Accuracy 0.6919\n",
      "Epoch 11 Batch 1700 Loss 1.4601 Accuracy 0.6918\n",
      "Epoch 11 Batch 1750 Loss 1.4603 Accuracy 0.6918\n",
      "Epoch 11 Batch 1800 Loss 1.4602 Accuracy 0.6918\n",
      "Epoch 11 Batch 1850 Loss 1.4608 Accuracy 0.6917\n",
      "Epoch 11 Batch 1900 Loss 1.4616 Accuracy 0.6916\n",
      "Epoch 11 Batch 1950 Loss 1.4623 Accuracy 0.6914\n",
      "Epoch 11 Batch 2000 Loss 1.4622 Accuracy 0.6915\n",
      "Epoch 11 Batch 2050 Loss 1.4626 Accuracy 0.6914\n",
      "Epoch 11 Batch 2100 Loss 1.4624 Accuracy 0.6915\n",
      "Epoch 11 Batch 2150 Loss 1.4629 Accuracy 0.6914\n",
      "Epoch 11 Batch 2200 Loss 1.4640 Accuracy 0.6912\n",
      "Epoch 11 Batch 2250 Loss 1.4650 Accuracy 0.6911\n",
      "Epoch 11 Batch 2300 Loss 1.4649 Accuracy 0.6911\n",
      "Epoch 11 Batch 2350 Loss 1.4656 Accuracy 0.6910\n",
      "Epoch 11 Batch 2400 Loss 1.4660 Accuracy 0.6910\n",
      "Epoch 11 Batch 2450 Loss 1.4664 Accuracy 0.6910\n",
      "Epoch 11 Batch 2500 Loss 1.4668 Accuracy 0.6909\n",
      "Epoch 11 Batch 2550 Loss 1.4670 Accuracy 0.6909\n",
      "Epoch 11 Batch 2600 Loss 1.4672 Accuracy 0.6909\n",
      "Epoch 11 Batch 2650 Loss 1.4677 Accuracy 0.6909\n",
      "Epoch 11 Batch 2700 Loss 1.4682 Accuracy 0.6908\n",
      "Epoch 11 Batch 2750 Loss 1.4687 Accuracy 0.6907\n",
      "Epoch 11 Batch 2800 Loss 1.4690 Accuracy 0.6907\n",
      "Epoch 11 Batch 2850 Loss 1.4697 Accuracy 0.6906\n",
      "Epoch 11 Batch 2900 Loss 1.4700 Accuracy 0.6905\n",
      "Epoch 11 Batch 2950 Loss 1.4703 Accuracy 0.6906\n",
      "Epoch 11 Batch 3000 Loss 1.4705 Accuracy 0.6905\n",
      "Epoch 11 Loss 1.4705 Accuracy 0.6905\n",
      "Time taken for 1 epoch: 242.23 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.4385 Accuracy 0.6862\n",
      "Epoch 12 Batch 50 Loss 1.3900 Accuracy 0.7023\n",
      "Epoch 12 Batch 100 Loss 1.4022 Accuracy 0.7008\n",
      "Epoch 12 Batch 150 Loss 1.4144 Accuracy 0.6982\n",
      "Epoch 12 Batch 200 Loss 1.4151 Accuracy 0.6983\n",
      "Epoch 12 Batch 250 Loss 1.4215 Accuracy 0.6975\n",
      "Epoch 12 Batch 300 Loss 1.4253 Accuracy 0.6968\n",
      "Epoch 12 Batch 350 Loss 1.4273 Accuracy 0.6964\n",
      "Epoch 12 Batch 400 Loss 1.4287 Accuracy 0.6962\n",
      "Epoch 12 Batch 450 Loss 1.4277 Accuracy 0.6962\n",
      "Epoch 12 Batch 500 Loss 1.4282 Accuracy 0.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 550 Loss 1.4292 Accuracy 0.6960\n",
      "Epoch 12 Batch 600 Loss 1.4312 Accuracy 0.6956\n",
      "Epoch 12 Batch 650 Loss 1.4306 Accuracy 0.6957\n",
      "Epoch 12 Batch 700 Loss 1.4321 Accuracy 0.6956\n",
      "Epoch 12 Batch 750 Loss 1.4334 Accuracy 0.6955\n",
      "Epoch 12 Batch 800 Loss 1.4349 Accuracy 0.6954\n",
      "Epoch 12 Batch 850 Loss 1.4368 Accuracy 0.6951\n",
      "Epoch 12 Batch 900 Loss 1.4368 Accuracy 0.6952\n",
      "Epoch 12 Batch 950 Loss 1.4383 Accuracy 0.6950\n",
      "Epoch 12 Batch 1000 Loss 1.4392 Accuracy 0.6949\n",
      "Epoch 12 Batch 1050 Loss 1.4400 Accuracy 0.6949\n",
      "Epoch 12 Batch 1100 Loss 1.4406 Accuracy 0.6949\n",
      "Epoch 12 Batch 1150 Loss 1.4417 Accuracy 0.6947\n",
      "Epoch 12 Batch 1200 Loss 1.4418 Accuracy 0.6947\n",
      "Epoch 12 Batch 1250 Loss 1.4425 Accuracy 0.6946\n",
      "Epoch 12 Batch 1300 Loss 1.4434 Accuracy 0.6945\n",
      "Epoch 12 Batch 1350 Loss 1.4439 Accuracy 0.6944\n",
      "Epoch 12 Batch 1400 Loss 1.4450 Accuracy 0.6943\n",
      "Epoch 12 Batch 1450 Loss 1.4456 Accuracy 0.6942\n",
      "Epoch 12 Batch 1500 Loss 1.4463 Accuracy 0.6941\n",
      "Epoch 12 Batch 1550 Loss 1.4473 Accuracy 0.6939\n",
      "Epoch 12 Batch 1600 Loss 1.4483 Accuracy 0.6938\n",
      "Epoch 12 Batch 1650 Loss 1.4488 Accuracy 0.6938\n",
      "Epoch 12 Batch 1700 Loss 1.4490 Accuracy 0.6938\n",
      "Epoch 12 Batch 1750 Loss 1.4501 Accuracy 0.6936\n",
      "Epoch 12 Batch 1800 Loss 1.4511 Accuracy 0.6934\n",
      "Epoch 12 Batch 1850 Loss 1.4519 Accuracy 0.6933\n",
      "Epoch 12 Batch 1900 Loss 1.4529 Accuracy 0.6933\n",
      "Epoch 12 Batch 1950 Loss 1.4537 Accuracy 0.6931\n",
      "Epoch 12 Batch 2000 Loss 1.4542 Accuracy 0.6930\n",
      "Epoch 12 Batch 2050 Loss 1.4550 Accuracy 0.6929\n",
      "Epoch 12 Batch 2100 Loss 1.4552 Accuracy 0.6928\n",
      "Epoch 12 Batch 2150 Loss 1.4555 Accuracy 0.6928\n",
      "Epoch 12 Batch 2200 Loss 1.4563 Accuracy 0.6927\n",
      "Epoch 12 Batch 2250 Loss 1.4569 Accuracy 0.6926\n",
      "Epoch 12 Batch 2300 Loss 1.4578 Accuracy 0.6925\n",
      "Epoch 12 Batch 2350 Loss 1.4584 Accuracy 0.6924\n",
      "Epoch 12 Batch 2400 Loss 1.4592 Accuracy 0.6924\n",
      "Epoch 12 Batch 2450 Loss 1.4599 Accuracy 0.6923\n",
      "Epoch 12 Batch 2500 Loss 1.4607 Accuracy 0.6922\n",
      "Epoch 12 Batch 2550 Loss 1.4609 Accuracy 0.6922\n",
      "Epoch 12 Batch 2600 Loss 1.4613 Accuracy 0.6922\n",
      "Epoch 12 Batch 2650 Loss 1.4616 Accuracy 0.6921\n",
      "Epoch 12 Batch 2700 Loss 1.4622 Accuracy 0.6920\n",
      "Epoch 12 Batch 2750 Loss 1.4632 Accuracy 0.6918\n",
      "Epoch 12 Batch 2800 Loss 1.4637 Accuracy 0.6918\n",
      "Epoch 12 Batch 2850 Loss 1.4636 Accuracy 0.6918\n",
      "Epoch 12 Batch 2900 Loss 1.4641 Accuracy 0.6918\n",
      "Epoch 12 Batch 2950 Loss 1.4649 Accuracy 0.6916\n",
      "Epoch 12 Batch 3000 Loss 1.4651 Accuracy 0.6916\n",
      "Epoch 12 Loss 1.4651 Accuracy 0.6916\n",
      "Time taken for 1 epoch: 244.48 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.3984 Accuracy 0.7084\n",
      "Epoch 13 Batch 50 Loss 1.3993 Accuracy 0.7011\n",
      "Epoch 13 Batch 100 Loss 1.4149 Accuracy 0.6987\n",
      "Epoch 13 Batch 150 Loss 1.4154 Accuracy 0.6986\n",
      "Epoch 13 Batch 200 Loss 1.4225 Accuracy 0.6978\n",
      "Epoch 13 Batch 250 Loss 1.4204 Accuracy 0.6980\n",
      "Epoch 13 Batch 300 Loss 1.4202 Accuracy 0.6984\n",
      "Epoch 13 Batch 350 Loss 1.4228 Accuracy 0.6979\n",
      "Epoch 13 Batch 400 Loss 1.4250 Accuracy 0.6974\n",
      "Epoch 13 Batch 450 Loss 1.4233 Accuracy 0.6976\n",
      "Epoch 13 Batch 500 Loss 1.4247 Accuracy 0.6973\n",
      "Epoch 13 Batch 550 Loss 1.4242 Accuracy 0.6975\n",
      "Epoch 13 Batch 600 Loss 1.4255 Accuracy 0.6972\n",
      "Epoch 13 Batch 650 Loss 1.4272 Accuracy 0.6968\n",
      "Epoch 13 Batch 700 Loss 1.4280 Accuracy 0.6967\n",
      "Epoch 13 Batch 750 Loss 1.4299 Accuracy 0.6964\n",
      "Epoch 13 Batch 800 Loss 1.4313 Accuracy 0.6962\n",
      "Epoch 13 Batch 850 Loss 1.4323 Accuracy 0.6961\n",
      "Epoch 13 Batch 900 Loss 1.4337 Accuracy 0.6959\n",
      "Epoch 13 Batch 950 Loss 1.4345 Accuracy 0.6957\n",
      "Epoch 13 Batch 1000 Loss 1.4359 Accuracy 0.6955\n",
      "Epoch 13 Batch 1050 Loss 1.4378 Accuracy 0.6952\n",
      "Epoch 13 Batch 1100 Loss 1.4391 Accuracy 0.6950\n",
      "Epoch 13 Batch 1150 Loss 1.4396 Accuracy 0.6949\n",
      "Epoch 13 Batch 1200 Loss 1.4408 Accuracy 0.6946\n",
      "Epoch 13 Batch 1250 Loss 1.4408 Accuracy 0.6947\n",
      "Epoch 13 Batch 1300 Loss 1.4412 Accuracy 0.6946\n",
      "Epoch 13 Batch 1350 Loss 1.4410 Accuracy 0.6947\n",
      "Epoch 13 Batch 1400 Loss 1.4415 Accuracy 0.6946\n",
      "Epoch 13 Batch 1450 Loss 1.4420 Accuracy 0.6946\n",
      "Epoch 13 Batch 1500 Loss 1.4434 Accuracy 0.6945\n",
      "Epoch 13 Batch 1550 Loss 1.4437 Accuracy 0.6944\n",
      "Epoch 13 Batch 1600 Loss 1.4440 Accuracy 0.6944\n",
      "Epoch 13 Batch 1650 Loss 1.4447 Accuracy 0.6943\n",
      "Epoch 13 Batch 1700 Loss 1.4452 Accuracy 0.6943\n",
      "Epoch 13 Batch 1750 Loss 1.4458 Accuracy 0.6942\n",
      "Epoch 13 Batch 1800 Loss 1.4462 Accuracy 0.6941\n",
      "Epoch 13 Batch 1850 Loss 1.4471 Accuracy 0.6940\n",
      "Epoch 13 Batch 1900 Loss 1.4477 Accuracy 0.6940\n",
      "Epoch 13 Batch 1950 Loss 1.4475 Accuracy 0.6940\n",
      "Epoch 13 Batch 2000 Loss 1.4482 Accuracy 0.6939\n",
      "Epoch 13 Batch 2050 Loss 1.4485 Accuracy 0.6939\n",
      "Epoch 13 Batch 2100 Loss 1.4487 Accuracy 0.6939\n",
      "Epoch 13 Batch 2150 Loss 1.4495 Accuracy 0.6938\n",
      "Epoch 13 Batch 2200 Loss 1.4504 Accuracy 0.6936\n",
      "Epoch 13 Batch 2250 Loss 1.4517 Accuracy 0.6934\n",
      "Epoch 13 Batch 2300 Loss 1.4522 Accuracy 0.6933\n",
      "Epoch 13 Batch 2350 Loss 1.4528 Accuracy 0.6932\n",
      "Epoch 13 Batch 2400 Loss 1.4530 Accuracy 0.6932\n",
      "Epoch 13 Batch 2450 Loss 1.4532 Accuracy 0.6932\n",
      "Epoch 13 Batch 2500 Loss 1.4536 Accuracy 0.6931\n",
      "Epoch 13 Batch 2550 Loss 1.4541 Accuracy 0.6931\n",
      "Epoch 13 Batch 2600 Loss 1.4547 Accuracy 0.6930\n",
      "Epoch 13 Batch 2650 Loss 1.4555 Accuracy 0.6929\n",
      "Epoch 13 Batch 2700 Loss 1.4561 Accuracy 0.6928\n",
      "Epoch 13 Batch 2750 Loss 1.4571 Accuracy 0.6926\n",
      "Epoch 13 Batch 2800 Loss 1.4575 Accuracy 0.6926\n",
      "Epoch 13 Batch 2850 Loss 1.4583 Accuracy 0.6925\n",
      "Epoch 13 Batch 2900 Loss 1.4588 Accuracy 0.6924\n",
      "Epoch 13 Batch 2950 Loss 1.4593 Accuracy 0.6923\n",
      "Epoch 13 Batch 3000 Loss 1.4597 Accuracy 0.6923\n",
      "Epoch 13 Loss 1.4597 Accuracy 0.6923\n",
      "Time taken for 1 epoch: 242.26 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.4618 Accuracy 0.7011\n",
      "Epoch 14 Batch 50 Loss 1.4022 Accuracy 0.7000\n",
      "Epoch 14 Batch 100 Loss 1.4001 Accuracy 0.7009\n",
      "Epoch 14 Batch 150 Loss 1.4049 Accuracy 0.7004\n",
      "Epoch 14 Batch 200 Loss 1.4095 Accuracy 0.6998\n",
      "Epoch 14 Batch 250 Loss 1.4115 Accuracy 0.6994\n",
      "Epoch 14 Batch 300 Loss 1.4166 Accuracy 0.6984\n",
      "Epoch 14 Batch 350 Loss 1.4154 Accuracy 0.6983\n",
      "Epoch 14 Batch 400 Loss 1.4183 Accuracy 0.6980\n",
      "Epoch 14 Batch 450 Loss 1.4207 Accuracy 0.6975\n",
      "Epoch 14 Batch 500 Loss 1.4228 Accuracy 0.6972\n",
      "Epoch 14 Batch 550 Loss 1.4245 Accuracy 0.6970\n",
      "Epoch 14 Batch 600 Loss 1.4248 Accuracy 0.6972\n",
      "Epoch 14 Batch 650 Loss 1.4253 Accuracy 0.6972\n",
      "Epoch 14 Batch 700 Loss 1.4270 Accuracy 0.6969\n",
      "Epoch 14 Batch 750 Loss 1.4288 Accuracy 0.6966\n",
      "Epoch 14 Batch 800 Loss 1.4297 Accuracy 0.6966\n",
      "Epoch 14 Batch 850 Loss 1.4303 Accuracy 0.6966\n",
      "Epoch 14 Batch 900 Loss 1.4313 Accuracy 0.6964\n",
      "Epoch 14 Batch 950 Loss 1.4317 Accuracy 0.6962\n",
      "Epoch 14 Batch 1000 Loss 1.4326 Accuracy 0.6960\n",
      "Epoch 14 Batch 1050 Loss 1.4340 Accuracy 0.6958\n",
      "Epoch 14 Batch 1100 Loss 1.4353 Accuracy 0.6956\n",
      "Epoch 14 Batch 1150 Loss 1.4366 Accuracy 0.6954\n",
      "Epoch 14 Batch 1200 Loss 1.4370 Accuracy 0.6953\n",
      "Epoch 14 Batch 1250 Loss 1.4367 Accuracy 0.6953\n",
      "Epoch 14 Batch 1300 Loss 1.4375 Accuracy 0.6952\n",
      "Epoch 14 Batch 1350 Loss 1.4375 Accuracy 0.6953\n",
      "Epoch 14 Batch 1400 Loss 1.4379 Accuracy 0.6953\n",
      "Epoch 14 Batch 1450 Loss 1.4383 Accuracy 0.6952\n",
      "Epoch 14 Batch 1500 Loss 1.4386 Accuracy 0.6952\n",
      "Epoch 14 Batch 1550 Loss 1.4394 Accuracy 0.6951\n",
      "Epoch 14 Batch 1600 Loss 1.4406 Accuracy 0.6949\n",
      "Epoch 14 Batch 1650 Loss 1.4406 Accuracy 0.6950\n",
      "Epoch 14 Batch 1700 Loss 1.4414 Accuracy 0.6949\n",
      "Epoch 14 Batch 1750 Loss 1.4423 Accuracy 0.6947\n",
      "Epoch 14 Batch 1800 Loss 1.4434 Accuracy 0.6945\n",
      "Epoch 14 Batch 1850 Loss 1.4438 Accuracy 0.6945\n",
      "Epoch 14 Batch 1900 Loss 1.4444 Accuracy 0.6944\n",
      "Epoch 14 Batch 1950 Loss 1.4454 Accuracy 0.6942\n",
      "Epoch 14 Batch 2000 Loss 1.4455 Accuracy 0.6942\n",
      "Epoch 14 Batch 2050 Loss 1.4464 Accuracy 0.6941\n",
      "Epoch 14 Batch 2100 Loss 1.4471 Accuracy 0.6940\n",
      "Epoch 14 Batch 2150 Loss 1.4481 Accuracy 0.6939\n",
      "Epoch 14 Batch 2200 Loss 1.4490 Accuracy 0.6937\n",
      "Epoch 14 Batch 2250 Loss 1.4490 Accuracy 0.6938\n",
      "Epoch 14 Batch 2300 Loss 1.4498 Accuracy 0.6937\n",
      "Epoch 14 Batch 2350 Loss 1.4500 Accuracy 0.6937\n",
      "Epoch 14 Batch 2400 Loss 1.4501 Accuracy 0.6937\n",
      "Epoch 14 Batch 2450 Loss 1.4507 Accuracy 0.6936\n",
      "Epoch 14 Batch 2500 Loss 1.4512 Accuracy 0.6936\n",
      "Epoch 14 Batch 2550 Loss 1.4514 Accuracy 0.6936\n",
      "Epoch 14 Batch 2600 Loss 1.4516 Accuracy 0.6936\n",
      "Epoch 14 Batch 2650 Loss 1.4523 Accuracy 0.6935\n",
      "Epoch 14 Batch 2700 Loss 1.4528 Accuracy 0.6934\n",
      "Epoch 14 Batch 2750 Loss 1.4534 Accuracy 0.6934\n",
      "Epoch 14 Batch 2800 Loss 1.4539 Accuracy 0.6933\n",
      "Epoch 14 Batch 2850 Loss 1.4548 Accuracy 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 2900 Loss 1.4555 Accuracy 0.6931\n",
      "Epoch 14 Batch 2950 Loss 1.4559 Accuracy 0.6930\n",
      "Epoch 14 Batch 3000 Loss 1.4556 Accuracy 0.6931\n",
      "Epoch 14 Loss 1.4556 Accuracy 0.6931\n",
      "Time taken for 1 epoch: 242.06 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.3721 Accuracy 0.7129\n",
      "Epoch 15 Batch 50 Loss 1.3996 Accuracy 0.7012\n",
      "Epoch 15 Batch 100 Loss 1.3995 Accuracy 0.7006\n",
      "Epoch 15 Batch 150 Loss 1.4064 Accuracy 0.6991\n",
      "Epoch 15 Batch 200 Loss 1.4034 Accuracy 0.7000\n",
      "Epoch 15 Batch 250 Loss 1.4038 Accuracy 0.7002\n",
      "Epoch 15 Batch 300 Loss 1.4075 Accuracy 0.6996\n",
      "Epoch 15 Batch 350 Loss 1.4094 Accuracy 0.6994\n",
      "Epoch 15 Batch 400 Loss 1.4105 Accuracy 0.6992\n",
      "Epoch 15 Batch 450 Loss 1.4100 Accuracy 0.6997\n",
      "Epoch 15 Batch 500 Loss 1.4105 Accuracy 0.6995\n",
      "Epoch 15 Batch 550 Loss 1.4101 Accuracy 0.6996\n",
      "Epoch 15 Batch 600 Loss 1.4115 Accuracy 0.6995\n",
      "Epoch 15 Batch 650 Loss 1.4138 Accuracy 0.6991\n",
      "Epoch 15 Batch 700 Loss 1.4173 Accuracy 0.6986\n",
      "Epoch 15 Batch 750 Loss 1.4185 Accuracy 0.6984\n",
      "Epoch 15 Batch 800 Loss 1.4188 Accuracy 0.6983\n",
      "Epoch 15 Batch 850 Loss 1.4193 Accuracy 0.6983\n",
      "Epoch 15 Batch 900 Loss 1.4204 Accuracy 0.6982\n",
      "Epoch 15 Batch 950 Loss 1.4227 Accuracy 0.6978\n",
      "Epoch 15 Batch 1000 Loss 1.4234 Accuracy 0.6977\n",
      "Epoch 15 Batch 1050 Loss 1.4243 Accuracy 0.6977\n",
      "Epoch 15 Batch 1100 Loss 1.4244 Accuracy 0.6977\n",
      "Epoch 15 Batch 1150 Loss 1.4255 Accuracy 0.6976\n",
      "Epoch 15 Batch 1200 Loss 1.4258 Accuracy 0.6975\n",
      "Epoch 15 Batch 1250 Loss 1.4274 Accuracy 0.6973\n",
      "Epoch 15 Batch 1300 Loss 1.4284 Accuracy 0.6971\n",
      "Epoch 15 Batch 1350 Loss 1.4293 Accuracy 0.6970\n",
      "Epoch 15 Batch 1400 Loss 1.4303 Accuracy 0.6969\n",
      "Epoch 15 Batch 1450 Loss 1.4311 Accuracy 0.6967\n",
      "Epoch 15 Batch 1500 Loss 1.4323 Accuracy 0.6965\n",
      "Epoch 15 Batch 1550 Loss 1.4335 Accuracy 0.6963\n",
      "Epoch 15 Batch 1600 Loss 1.4346 Accuracy 0.6961\n",
      "Epoch 15 Batch 1650 Loss 1.4359 Accuracy 0.6959\n",
      "Epoch 15 Batch 1700 Loss 1.4361 Accuracy 0.6959\n",
      "Epoch 15 Batch 1750 Loss 1.4373 Accuracy 0.6957\n",
      "Epoch 15 Batch 1800 Loss 1.4377 Accuracy 0.6957\n",
      "Epoch 15 Batch 1850 Loss 1.4385 Accuracy 0.6956\n",
      "Epoch 15 Batch 1900 Loss 1.4389 Accuracy 0.6955\n",
      "Epoch 15 Batch 1950 Loss 1.4398 Accuracy 0.6954\n",
      "Epoch 15 Batch 2000 Loss 1.4401 Accuracy 0.6953\n",
      "Epoch 15 Batch 2050 Loss 1.4409 Accuracy 0.6952\n",
      "Epoch 15 Batch 2100 Loss 1.4415 Accuracy 0.6951\n",
      "Epoch 15 Batch 2150 Loss 1.4422 Accuracy 0.6950\n",
      "Epoch 15 Batch 2200 Loss 1.4427 Accuracy 0.6949\n",
      "Epoch 15 Batch 2250 Loss 1.4435 Accuracy 0.6948\n",
      "Epoch 15 Batch 2300 Loss 1.4440 Accuracy 0.6947\n",
      "Epoch 15 Batch 2350 Loss 1.4443 Accuracy 0.6946\n",
      "Epoch 15 Batch 2400 Loss 1.4449 Accuracy 0.6946\n",
      "Epoch 15 Batch 2450 Loss 1.4453 Accuracy 0.6945\n",
      "Epoch 15 Batch 2500 Loss 1.4460 Accuracy 0.6944\n",
      "Epoch 15 Batch 2550 Loss 1.4461 Accuracy 0.6944\n",
      "Epoch 15 Batch 2600 Loss 1.4468 Accuracy 0.6943\n",
      "Epoch 15 Batch 2650 Loss 1.4467 Accuracy 0.6943\n",
      "Epoch 15 Batch 2700 Loss 1.4475 Accuracy 0.6942\n",
      "Epoch 15 Batch 2750 Loss 1.4478 Accuracy 0.6942\n",
      "Epoch 15 Batch 2800 Loss 1.4481 Accuracy 0.6942\n",
      "Epoch 15 Batch 2850 Loss 1.4486 Accuracy 0.6941\n",
      "Epoch 15 Batch 2900 Loss 1.4492 Accuracy 0.6940\n",
      "Epoch 15 Batch 2950 Loss 1.4498 Accuracy 0.6939\n",
      "Epoch 15 Batch 3000 Loss 1.4502 Accuracy 0.6939\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-7\n",
      "Epoch 15 Loss 1.4502 Accuracy 0.6938\n",
      "Time taken for 1 epoch: 243.75 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.4889 Accuracy 0.6856\n",
      "Epoch 16 Batch 50 Loss 1.4010 Accuracy 0.7020\n",
      "Epoch 16 Batch 100 Loss 1.3996 Accuracy 0.7026\n",
      "Epoch 16 Batch 150 Loss 1.4014 Accuracy 0.7014\n",
      "Epoch 16 Batch 200 Loss 1.4027 Accuracy 0.7010\n",
      "Epoch 16 Batch 250 Loss 1.4040 Accuracy 0.7006\n",
      "Epoch 16 Batch 300 Loss 1.4033 Accuracy 0.7007\n",
      "Epoch 16 Batch 350 Loss 1.4042 Accuracy 0.7007\n",
      "Epoch 16 Batch 400 Loss 1.4062 Accuracy 0.7004\n",
      "Epoch 16 Batch 450 Loss 1.4083 Accuracy 0.7002\n",
      "Epoch 16 Batch 500 Loss 1.4112 Accuracy 0.6998\n",
      "Epoch 16 Batch 550 Loss 1.4118 Accuracy 0.6996\n",
      "Epoch 16 Batch 600 Loss 1.4100 Accuracy 0.6998\n",
      "Epoch 16 Batch 650 Loss 1.4122 Accuracy 0.6995\n",
      "Epoch 16 Batch 700 Loss 1.4141 Accuracy 0.6991\n",
      "Epoch 16 Batch 750 Loss 1.4172 Accuracy 0.6985\n",
      "Epoch 16 Batch 800 Loss 1.4179 Accuracy 0.6983\n",
      "Epoch 16 Batch 850 Loss 1.4179 Accuracy 0.6984\n",
      "Epoch 16 Batch 900 Loss 1.4186 Accuracy 0.6983\n",
      "Epoch 16 Batch 950 Loss 1.4202 Accuracy 0.6979\n",
      "Epoch 16 Batch 1000 Loss 1.4208 Accuracy 0.6979\n",
      "Epoch 16 Batch 1050 Loss 1.4208 Accuracy 0.6979\n",
      "Epoch 16 Batch 1100 Loss 1.4220 Accuracy 0.6978\n",
      "Epoch 16 Batch 1150 Loss 1.4236 Accuracy 0.6976\n",
      "Epoch 16 Batch 1200 Loss 1.4244 Accuracy 0.6975\n",
      "Epoch 16 Batch 1250 Loss 1.4252 Accuracy 0.6975\n",
      "Epoch 16 Batch 1300 Loss 1.4256 Accuracy 0.6974\n",
      "Epoch 16 Batch 1350 Loss 1.4267 Accuracy 0.6972\n",
      "Epoch 16 Batch 1400 Loss 1.4283 Accuracy 0.6969\n",
      "Epoch 16 Batch 1450 Loss 1.4291 Accuracy 0.6968\n",
      "Epoch 16 Batch 1500 Loss 1.4308 Accuracy 0.6966\n",
      "Epoch 16 Batch 1550 Loss 1.4322 Accuracy 0.6963\n",
      "Epoch 16 Batch 1600 Loss 1.4331 Accuracy 0.6962\n",
      "Epoch 16 Batch 1650 Loss 1.4339 Accuracy 0.6961\n",
      "Epoch 16 Batch 1700 Loss 1.4349 Accuracy 0.6960\n",
      "Epoch 16 Batch 1750 Loss 1.4351 Accuracy 0.6959\n",
      "Epoch 16 Batch 1800 Loss 1.4357 Accuracy 0.6958\n",
      "Epoch 16 Batch 1850 Loss 1.4363 Accuracy 0.6957\n",
      "Epoch 16 Batch 1900 Loss 1.4367 Accuracy 0.6956\n",
      "Epoch 16 Batch 1950 Loss 1.4372 Accuracy 0.6956\n",
      "Epoch 16 Batch 2000 Loss 1.4379 Accuracy 0.6955\n",
      "Epoch 16 Batch 2050 Loss 1.4378 Accuracy 0.6955\n",
      "Epoch 16 Batch 2100 Loss 1.4384 Accuracy 0.6955\n",
      "Epoch 16 Batch 2150 Loss 1.4386 Accuracy 0.6954\n",
      "Epoch 16 Batch 2200 Loss 1.4395 Accuracy 0.6953\n",
      "Epoch 16 Batch 2250 Loss 1.4399 Accuracy 0.6953\n",
      "Epoch 16 Batch 2300 Loss 1.4401 Accuracy 0.6953\n",
      "Epoch 16 Batch 2350 Loss 1.4409 Accuracy 0.6952\n",
      "Epoch 16 Batch 2400 Loss 1.4416 Accuracy 0.6950\n",
      "Epoch 16 Batch 2450 Loss 1.4420 Accuracy 0.6949\n",
      "Epoch 16 Batch 2500 Loss 1.4421 Accuracy 0.6950\n",
      "Epoch 16 Batch 2550 Loss 1.4422 Accuracy 0.6950\n",
      "Epoch 16 Batch 2600 Loss 1.4428 Accuracy 0.6949\n",
      "Epoch 16 Batch 2650 Loss 1.4432 Accuracy 0.6949\n",
      "Epoch 16 Batch 2700 Loss 1.4438 Accuracy 0.6948\n",
      "Epoch 16 Batch 2750 Loss 1.4441 Accuracy 0.6948\n",
      "Epoch 16 Batch 2800 Loss 1.4452 Accuracy 0.6946\n",
      "Epoch 16 Batch 2850 Loss 1.4453 Accuracy 0.6946\n",
      "Epoch 16 Batch 2900 Loss 1.4456 Accuracy 0.6946\n",
      "Epoch 16 Batch 2950 Loss 1.4457 Accuracy 0.6945\n",
      "Epoch 16 Batch 3000 Loss 1.4458 Accuracy 0.6945\n",
      "Epoch 16 Loss 1.4458 Accuracy 0.6945\n",
      "Time taken for 1 epoch: 243.87 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.5949 Accuracy 0.6626\n",
      "Epoch 17 Batch 50 Loss 1.4190 Accuracy 0.6981\n",
      "Epoch 17 Batch 100 Loss 1.4147 Accuracy 0.6985\n",
      "Epoch 17 Batch 150 Loss 1.4100 Accuracy 0.6985\n",
      "Epoch 17 Batch 200 Loss 1.4052 Accuracy 0.6994\n",
      "Epoch 17 Batch 250 Loss 1.4055 Accuracy 0.6994\n",
      "Epoch 17 Batch 300 Loss 1.4047 Accuracy 0.6999\n",
      "Epoch 17 Batch 350 Loss 1.4061 Accuracy 0.6997\n",
      "Epoch 17 Batch 400 Loss 1.4079 Accuracy 0.6993\n",
      "Epoch 17 Batch 450 Loss 1.4092 Accuracy 0.6992\n",
      "Epoch 17 Batch 500 Loss 1.4136 Accuracy 0.6984\n",
      "Epoch 17 Batch 550 Loss 1.4157 Accuracy 0.6981\n",
      "Epoch 17 Batch 600 Loss 1.4152 Accuracy 0.6982\n",
      "Epoch 17 Batch 650 Loss 1.4147 Accuracy 0.6985\n",
      "Epoch 17 Batch 700 Loss 1.4162 Accuracy 0.6982\n",
      "Epoch 17 Batch 750 Loss 1.4186 Accuracy 0.6978\n",
      "Epoch 17 Batch 800 Loss 1.4194 Accuracy 0.6977\n",
      "Epoch 17 Batch 850 Loss 1.4198 Accuracy 0.6978\n",
      "Epoch 17 Batch 900 Loss 1.4209 Accuracy 0.6976\n",
      "Epoch 17 Batch 950 Loss 1.4223 Accuracy 0.6975\n",
      "Epoch 17 Batch 1000 Loss 1.4220 Accuracy 0.6975\n",
      "Epoch 17 Batch 1050 Loss 1.4229 Accuracy 0.6974\n",
      "Epoch 17 Batch 1100 Loss 1.4229 Accuracy 0.6975\n",
      "Epoch 17 Batch 1150 Loss 1.4238 Accuracy 0.6973\n",
      "Epoch 17 Batch 1200 Loss 1.4248 Accuracy 0.6972\n",
      "Epoch 17 Batch 1250 Loss 1.4262 Accuracy 0.6970\n",
      "Epoch 17 Batch 1300 Loss 1.4262 Accuracy 0.6970\n",
      "Epoch 17 Batch 1350 Loss 1.4275 Accuracy 0.6968\n",
      "Epoch 17 Batch 1400 Loss 1.4279 Accuracy 0.6967\n",
      "Epoch 17 Batch 1450 Loss 1.4296 Accuracy 0.6965\n",
      "Epoch 17 Batch 1500 Loss 1.4294 Accuracy 0.6966\n",
      "Epoch 17 Batch 1550 Loss 1.4297 Accuracy 0.6965\n",
      "Epoch 17 Batch 1600 Loss 1.4298 Accuracy 0.6965\n",
      "Epoch 17 Batch 1650 Loss 1.4298 Accuracy 0.6965\n",
      "Epoch 17 Batch 1700 Loss 1.4304 Accuracy 0.6964\n",
      "Epoch 17 Batch 1750 Loss 1.4308 Accuracy 0.6964\n",
      "Epoch 17 Batch 1800 Loss 1.4313 Accuracy 0.6963\n",
      "Epoch 17 Batch 1850 Loss 1.4314 Accuracy 0.6963\n",
      "Epoch 17 Batch 1900 Loss 1.4319 Accuracy 0.6963\n",
      "Epoch 17 Batch 1950 Loss 1.4325 Accuracy 0.6962\n",
      "Epoch 17 Batch 2000 Loss 1.4333 Accuracy 0.6960\n",
      "Epoch 17 Batch 2050 Loss 1.4342 Accuracy 0.6959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 2100 Loss 1.4351 Accuracy 0.6958\n",
      "Epoch 17 Batch 2150 Loss 1.4353 Accuracy 0.6958\n",
      "Epoch 17 Batch 2200 Loss 1.4353 Accuracy 0.6958\n",
      "Epoch 17 Batch 2250 Loss 1.4359 Accuracy 0.6957\n",
      "Epoch 17 Batch 2300 Loss 1.4363 Accuracy 0.6957\n",
      "Epoch 17 Batch 2350 Loss 1.4364 Accuracy 0.6957\n",
      "Epoch 17 Batch 2400 Loss 1.4368 Accuracy 0.6956\n",
      "Epoch 17 Batch 2450 Loss 1.4371 Accuracy 0.6956\n",
      "Epoch 17 Batch 2500 Loss 1.4373 Accuracy 0.6956\n",
      "Epoch 17 Batch 2550 Loss 1.4378 Accuracy 0.6955\n",
      "Epoch 17 Batch 2600 Loss 1.4382 Accuracy 0.6954\n",
      "Epoch 17 Batch 2650 Loss 1.4387 Accuracy 0.6954\n",
      "Epoch 17 Batch 2700 Loss 1.4392 Accuracy 0.6953\n",
      "Epoch 17 Batch 2750 Loss 1.4398 Accuracy 0.6953\n",
      "Epoch 17 Batch 2800 Loss 1.4399 Accuracy 0.6953\n",
      "Epoch 17 Batch 2850 Loss 1.4402 Accuracy 0.6952\n",
      "Epoch 17 Batch 2900 Loss 1.4404 Accuracy 0.6952\n",
      "Epoch 17 Batch 2950 Loss 1.4408 Accuracy 0.6952\n",
      "Epoch 17 Batch 3000 Loss 1.4413 Accuracy 0.6951\n",
      "Epoch 17 Loss 1.4414 Accuracy 0.6951\n",
      "Time taken for 1 epoch: 242.75 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.4044 Accuracy 0.6965\n",
      "Epoch 18 Batch 50 Loss 1.3889 Accuracy 0.7039\n",
      "Epoch 18 Batch 100 Loss 1.3982 Accuracy 0.7008\n",
      "Epoch 18 Batch 150 Loss 1.3946 Accuracy 0.7017\n",
      "Epoch 18 Batch 200 Loss 1.3980 Accuracy 0.7010\n",
      "Epoch 18 Batch 250 Loss 1.4033 Accuracy 0.7005\n",
      "Epoch 18 Batch 300 Loss 1.4017 Accuracy 0.7008\n",
      "Epoch 18 Batch 350 Loss 1.4034 Accuracy 0.7008\n",
      "Epoch 18 Batch 400 Loss 1.4049 Accuracy 0.7007\n",
      "Epoch 18 Batch 450 Loss 1.4025 Accuracy 0.7008\n",
      "Epoch 18 Batch 500 Loss 1.4043 Accuracy 0.7007\n",
      "Epoch 18 Batch 550 Loss 1.4048 Accuracy 0.7007\n",
      "Epoch 18 Batch 600 Loss 1.4052 Accuracy 0.7004\n",
      "Epoch 18 Batch 650 Loss 1.4076 Accuracy 0.6999\n",
      "Epoch 18 Batch 700 Loss 1.4093 Accuracy 0.6996\n",
      "Epoch 18 Batch 750 Loss 1.4090 Accuracy 0.6996\n",
      "Epoch 18 Batch 800 Loss 1.4096 Accuracy 0.6997\n",
      "Epoch 18 Batch 850 Loss 1.4107 Accuracy 0.6996\n",
      "Epoch 18 Batch 900 Loss 1.4125 Accuracy 0.6993\n",
      "Epoch 18 Batch 950 Loss 1.4148 Accuracy 0.6989\n",
      "Epoch 18 Batch 1000 Loss 1.4149 Accuracy 0.6989\n",
      "Epoch 18 Batch 1050 Loss 1.4156 Accuracy 0.6988\n",
      "Epoch 18 Batch 1100 Loss 1.4167 Accuracy 0.6986\n",
      "Epoch 18 Batch 1150 Loss 1.4171 Accuracy 0.6985\n",
      "Epoch 18 Batch 1200 Loss 1.4181 Accuracy 0.6983\n",
      "Epoch 18 Batch 1250 Loss 1.4182 Accuracy 0.6983\n",
      "Epoch 18 Batch 1300 Loss 1.4187 Accuracy 0.6982\n",
      "Epoch 18 Batch 1350 Loss 1.4203 Accuracy 0.6980\n",
      "Epoch 18 Batch 1400 Loss 1.4219 Accuracy 0.6977\n",
      "Epoch 18 Batch 1450 Loss 1.4222 Accuracy 0.6977\n",
      "Epoch 18 Batch 1500 Loss 1.4235 Accuracy 0.6975\n",
      "Epoch 18 Batch 1550 Loss 1.4245 Accuracy 0.6974\n",
      "Epoch 18 Batch 1600 Loss 1.4243 Accuracy 0.6974\n",
      "Epoch 18 Batch 1650 Loss 1.4248 Accuracy 0.6974\n",
      "Epoch 18 Batch 1700 Loss 1.4255 Accuracy 0.6973\n",
      "Epoch 18 Batch 1750 Loss 1.4257 Accuracy 0.6973\n",
      "Epoch 18 Batch 1800 Loss 1.4265 Accuracy 0.6972\n",
      "Epoch 18 Batch 1850 Loss 1.4267 Accuracy 0.6973\n",
      "Epoch 18 Batch 1900 Loss 1.4278 Accuracy 0.6970\n",
      "Epoch 18 Batch 1950 Loss 1.4280 Accuracy 0.6970\n",
      "Epoch 18 Batch 2000 Loss 1.4285 Accuracy 0.6970\n",
      "Epoch 18 Batch 2050 Loss 1.4290 Accuracy 0.6969\n",
      "Epoch 18 Batch 2100 Loss 1.4295 Accuracy 0.6968\n",
      "Epoch 18 Batch 2150 Loss 1.4299 Accuracy 0.6968\n",
      "Epoch 18 Batch 2200 Loss 1.4306 Accuracy 0.6966\n",
      "Epoch 18 Batch 2250 Loss 1.4306 Accuracy 0.6967\n",
      "Epoch 18 Batch 2300 Loss 1.4311 Accuracy 0.6966\n",
      "Epoch 18 Batch 2350 Loss 1.4320 Accuracy 0.6965\n",
      "Epoch 18 Batch 2400 Loss 1.4327 Accuracy 0.6964\n",
      "Epoch 18 Batch 2450 Loss 1.4338 Accuracy 0.6963\n",
      "Epoch 18 Batch 2500 Loss 1.4340 Accuracy 0.6962\n",
      "Epoch 18 Batch 2550 Loss 1.4344 Accuracy 0.6962\n",
      "Epoch 18 Batch 2600 Loss 1.4347 Accuracy 0.6961\n",
      "Epoch 18 Batch 2650 Loss 1.4350 Accuracy 0.6961\n",
      "Epoch 18 Batch 2700 Loss 1.4351 Accuracy 0.6961\n",
      "Epoch 18 Batch 2750 Loss 1.4354 Accuracy 0.6961\n",
      "Epoch 18 Batch 2800 Loss 1.4355 Accuracy 0.6961\n",
      "Epoch 18 Batch 2850 Loss 1.4360 Accuracy 0.6960\n",
      "Epoch 18 Batch 2900 Loss 1.4366 Accuracy 0.6960\n",
      "Epoch 18 Batch 2950 Loss 1.4369 Accuracy 0.6959\n",
      "Epoch 18 Batch 3000 Loss 1.4374 Accuracy 0.6959\n",
      "Epoch 18 Loss 1.4375 Accuracy 0.6958\n",
      "Time taken for 1 epoch: 242.09 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.3996 Accuracy 0.7052\n",
      "Epoch 19 Batch 50 Loss 1.3811 Accuracy 0.7034\n",
      "Epoch 19 Batch 100 Loss 1.3889 Accuracy 0.7018\n",
      "Epoch 19 Batch 150 Loss 1.3955 Accuracy 0.7009\n",
      "Epoch 19 Batch 200 Loss 1.3933 Accuracy 0.7018\n",
      "Epoch 19 Batch 250 Loss 1.3939 Accuracy 0.7016\n",
      "Epoch 19 Batch 300 Loss 1.3955 Accuracy 0.7017\n",
      "Epoch 19 Batch 350 Loss 1.3992 Accuracy 0.7010\n",
      "Epoch 19 Batch 400 Loss 1.3976 Accuracy 0.7015\n",
      "Epoch 19 Batch 450 Loss 1.3988 Accuracy 0.7014\n",
      "Epoch 19 Batch 500 Loss 1.4011 Accuracy 0.7012\n",
      "Epoch 19 Batch 550 Loss 1.4015 Accuracy 0.7010\n",
      "Epoch 19 Batch 600 Loss 1.4022 Accuracy 0.7009\n",
      "Epoch 19 Batch 650 Loss 1.4024 Accuracy 0.7009\n",
      "Epoch 19 Batch 700 Loss 1.4033 Accuracy 0.7007\n",
      "Epoch 19 Batch 750 Loss 1.4055 Accuracy 0.7003\n",
      "Epoch 19 Batch 800 Loss 1.4050 Accuracy 0.7005\n",
      "Epoch 19 Batch 850 Loss 1.4053 Accuracy 0.7004\n",
      "Epoch 19 Batch 900 Loss 1.4061 Accuracy 0.7003\n",
      "Epoch 19 Batch 950 Loss 1.4086 Accuracy 0.6997\n",
      "Epoch 19 Batch 1000 Loss 1.4101 Accuracy 0.6995\n",
      "Epoch 19 Batch 1050 Loss 1.4114 Accuracy 0.6993\n",
      "Epoch 19 Batch 1100 Loss 1.4120 Accuracy 0.6992\n",
      "Epoch 19 Batch 1150 Loss 1.4131 Accuracy 0.6990\n",
      "Epoch 19 Batch 1200 Loss 1.4141 Accuracy 0.6988\n",
      "Epoch 19 Batch 1250 Loss 1.4150 Accuracy 0.6987\n",
      "Epoch 19 Batch 1300 Loss 1.4156 Accuracy 0.6986\n",
      "Epoch 19 Batch 1350 Loss 1.4164 Accuracy 0.6984\n",
      "Epoch 19 Batch 1400 Loss 1.4168 Accuracy 0.6983\n",
      "Epoch 19 Batch 1450 Loss 1.4167 Accuracy 0.6984\n",
      "Epoch 19 Batch 1500 Loss 1.4172 Accuracy 0.6983\n",
      "Epoch 19 Batch 1550 Loss 1.4172 Accuracy 0.6983\n",
      "Epoch 19 Batch 1600 Loss 1.4172 Accuracy 0.6983\n",
      "Epoch 19 Batch 1650 Loss 1.4179 Accuracy 0.6982\n",
      "Epoch 19 Batch 1700 Loss 1.4183 Accuracy 0.6982\n",
      "Epoch 19 Batch 1750 Loss 1.4189 Accuracy 0.6981\n",
      "Epoch 19 Batch 1800 Loss 1.4196 Accuracy 0.6980\n",
      "Epoch 19 Batch 1850 Loss 1.4206 Accuracy 0.6978\n",
      "Epoch 19 Batch 1900 Loss 1.4213 Accuracy 0.6978\n",
      "Epoch 19 Batch 1950 Loss 1.4225 Accuracy 0.6976\n",
      "Epoch 19 Batch 2000 Loss 1.4232 Accuracy 0.6975\n",
      "Epoch 19 Batch 2050 Loss 1.4240 Accuracy 0.6975\n",
      "Epoch 19 Batch 2100 Loss 1.4245 Accuracy 0.6974\n",
      "Epoch 19 Batch 2150 Loss 1.4250 Accuracy 0.6974\n",
      "Epoch 19 Batch 2200 Loss 1.4259 Accuracy 0.6972\n",
      "Epoch 19 Batch 2250 Loss 1.4266 Accuracy 0.6971\n",
      "Epoch 19 Batch 2300 Loss 1.4274 Accuracy 0.6970\n",
      "Epoch 19 Batch 2350 Loss 1.4279 Accuracy 0.6969\n",
      "Epoch 19 Batch 2400 Loss 1.4281 Accuracy 0.6969\n",
      "Epoch 19 Batch 2450 Loss 1.4288 Accuracy 0.6968\n",
      "Epoch 19 Batch 2500 Loss 1.4292 Accuracy 0.6968\n",
      "Epoch 19 Batch 2550 Loss 1.4295 Accuracy 0.6968\n",
      "Epoch 19 Batch 2600 Loss 1.4303 Accuracy 0.6967\n",
      "Epoch 19 Batch 2650 Loss 1.4305 Accuracy 0.6967\n",
      "Epoch 19 Batch 2700 Loss 1.4310 Accuracy 0.6966\n",
      "Epoch 19 Batch 2750 Loss 1.4313 Accuracy 0.6966\n",
      "Epoch 19 Batch 2800 Loss 1.4314 Accuracy 0.6966\n",
      "Epoch 19 Batch 2850 Loss 1.4320 Accuracy 0.6965\n",
      "Epoch 19 Batch 2900 Loss 1.4324 Accuracy 0.6965\n",
      "Epoch 19 Batch 2950 Loss 1.4330 Accuracy 0.6965\n",
      "Epoch 19 Batch 3000 Loss 1.4337 Accuracy 0.6963\n",
      "Epoch 19 Loss 1.4337 Accuracy 0.6964\n",
      "Time taken for 1 epoch: 243.96 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.3942 Accuracy 0.7046\n",
      "Epoch 20 Batch 50 Loss 1.3706 Accuracy 0.7047\n",
      "Epoch 20 Batch 100 Loss 1.3744 Accuracy 0.7044\n",
      "Epoch 20 Batch 150 Loss 1.3768 Accuracy 0.7045\n",
      "Epoch 20 Batch 200 Loss 1.3833 Accuracy 0.7034\n",
      "Epoch 20 Batch 250 Loss 1.3869 Accuracy 0.7032\n",
      "Epoch 20 Batch 300 Loss 1.3870 Accuracy 0.7031\n",
      "Epoch 20 Batch 350 Loss 1.3915 Accuracy 0.7024\n",
      "Epoch 20 Batch 400 Loss 1.3907 Accuracy 0.7026\n",
      "Epoch 20 Batch 450 Loss 1.3959 Accuracy 0.7016\n",
      "Epoch 20 Batch 500 Loss 1.3977 Accuracy 0.7012\n",
      "Epoch 20 Batch 550 Loss 1.3977 Accuracy 0.7012\n",
      "Epoch 20 Batch 600 Loss 1.4002 Accuracy 0.7010\n",
      "Epoch 20 Batch 650 Loss 1.4016 Accuracy 0.7008\n",
      "Epoch 20 Batch 700 Loss 1.4033 Accuracy 0.7005\n",
      "Epoch 20 Batch 750 Loss 1.4030 Accuracy 0.7006\n",
      "Epoch 20 Batch 800 Loss 1.4048 Accuracy 0.7004\n",
      "Epoch 20 Batch 850 Loss 1.4068 Accuracy 0.7001\n",
      "Epoch 20 Batch 900 Loss 1.4074 Accuracy 0.7000\n",
      "Epoch 20 Batch 950 Loss 1.4079 Accuracy 0.6999\n",
      "Epoch 20 Batch 1000 Loss 1.4094 Accuracy 0.6997\n",
      "Epoch 20 Batch 1050 Loss 1.4100 Accuracy 0.6996\n",
      "Epoch 20 Batch 1100 Loss 1.4103 Accuracy 0.6996\n",
      "Epoch 20 Batch 1150 Loss 1.4107 Accuracy 0.6996\n",
      "Epoch 20 Batch 1200 Loss 1.4117 Accuracy 0.6994\n",
      "Epoch 20 Batch 1250 Loss 1.4136 Accuracy 0.6991\n",
      "Epoch 20 Batch 1300 Loss 1.4147 Accuracy 0.6990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 1350 Loss 1.4153 Accuracy 0.6989\n",
      "Epoch 20 Batch 1400 Loss 1.4155 Accuracy 0.6989\n",
      "Epoch 20 Batch 1450 Loss 1.4158 Accuracy 0.6989\n",
      "Epoch 20 Batch 1500 Loss 1.4158 Accuracy 0.6989\n",
      "Epoch 20 Batch 1550 Loss 1.4162 Accuracy 0.6988\n",
      "Epoch 20 Batch 1600 Loss 1.4175 Accuracy 0.6986\n",
      "Epoch 20 Batch 1650 Loss 1.4173 Accuracy 0.6987\n",
      "Epoch 20 Batch 1700 Loss 1.4182 Accuracy 0.6986\n",
      "Epoch 20 Batch 1750 Loss 1.4188 Accuracy 0.6986\n",
      "Epoch 20 Batch 1800 Loss 1.4197 Accuracy 0.6984\n",
      "Epoch 20 Batch 1850 Loss 1.4204 Accuracy 0.6983\n",
      "Epoch 20 Batch 1900 Loss 1.4211 Accuracy 0.6982\n",
      "Epoch 20 Batch 1950 Loss 1.4206 Accuracy 0.6983\n",
      "Epoch 20 Batch 2000 Loss 1.4211 Accuracy 0.6983\n",
      "Epoch 20 Batch 2050 Loss 1.4218 Accuracy 0.6982\n",
      "Epoch 20 Batch 2100 Loss 1.4224 Accuracy 0.6981\n",
      "Epoch 20 Batch 2150 Loss 1.4225 Accuracy 0.6981\n",
      "Epoch 20 Batch 2200 Loss 1.4226 Accuracy 0.6980\n",
      "Epoch 20 Batch 2250 Loss 1.4237 Accuracy 0.6979\n",
      "Epoch 20 Batch 2300 Loss 1.4239 Accuracy 0.6980\n",
      "Epoch 20 Batch 2350 Loss 1.4240 Accuracy 0.6979\n",
      "Epoch 20 Batch 2400 Loss 1.4249 Accuracy 0.6978\n",
      "Epoch 20 Batch 2450 Loss 1.4254 Accuracy 0.6978\n",
      "Epoch 20 Batch 2500 Loss 1.4260 Accuracy 0.6977\n",
      "Epoch 20 Batch 2550 Loss 1.4265 Accuracy 0.6976\n",
      "Epoch 20 Batch 2600 Loss 1.4268 Accuracy 0.6976\n",
      "Epoch 20 Batch 2650 Loss 1.4271 Accuracy 0.6976\n",
      "Epoch 20 Batch 2700 Loss 1.4284 Accuracy 0.6974\n",
      "Epoch 20 Batch 2750 Loss 1.4285 Accuracy 0.6973\n",
      "Epoch 20 Batch 2800 Loss 1.4289 Accuracy 0.6973\n",
      "Epoch 20 Batch 2850 Loss 1.4293 Accuracy 0.6972\n",
      "Epoch 20 Batch 2900 Loss 1.4298 Accuracy 0.6971\n",
      "Epoch 20 Batch 2950 Loss 1.4300 Accuracy 0.6971\n",
      "Epoch 20 Batch 3000 Loss 1.4302 Accuracy 0.6971\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-8\n",
      "Epoch 20 Loss 1.4301 Accuracy 0.6971\n",
      "Time taken for 1 epoch: 243.08 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-template",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "serious-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizers, transformer):\n",
    "        self.tokenizers = tokenizers\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=20):\n",
    "        # input sentence is french, hence adding the start and end token\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizers.fr.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # as the target is english, the first token to the transformer should be the\n",
    "        # english start token.\n",
    "        start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a python list) so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # select the last token from the seq_len dimension\n",
    "            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # concatentate the predicted_id to the output which is given to the decoder\n",
    "            # as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # output.shape (1, tokens)\n",
    "        text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
    "\n",
    "        tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop. So recalculate them outside\n",
    "        # the loop.\n",
    "        _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "lesbian-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "prescribed-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "colonial-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : ceci est un problème que nous devons résoudre.\n",
      "Prediction     : this is a problem we have to solve .\n",
      "Ground truth   : this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"ceci est un problème que nous devons résoudre.\"\n",
    "ground_truth = \"this is a problem we have to solve .\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-radar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
